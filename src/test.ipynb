{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">╭───────────────────────────────────────────────────── INFO ──────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Calling pdf_parser</span>                                                                                              <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33m╭─\u001b[0m\u001b[33m────────────────────────────────────────────────────\u001b[0m\u001b[33m INFO \u001b[0m\u001b[33m─────────────────────────────────────────────────────\u001b[0m\u001b[33m─╮\u001b[0m\n",
       "\u001b[33m│\u001b[0m \u001b[37mCalling pdf_parser\u001b[0m                                                                                              \u001b[33m│\u001b[0m\n",
       "\u001b[33m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">╭───────────────────────────────────────────────────── INFO ──────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Extracted 31 pages. First page preview:</span>                                                                         <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">1</span>                                                                                                               <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">LLM Post-Training: A Deep Dive into Reasoning</span>                                                                   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Large Language Models</span>                                                                                           <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Komal Kumar∗, Tajamul Ashraf∗,...</span>                                                                               <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33m╭─\u001b[0m\u001b[33m────────────────────────────────────────────────────\u001b[0m\u001b[33m INFO \u001b[0m\u001b[33m─────────────────────────────────────────────────────\u001b[0m\u001b[33m─╮\u001b[0m\n",
       "\u001b[33m│\u001b[0m \u001b[37mExtracted 31 pages. First page preview:\u001b[0m                                                                         \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m \u001b[37m1\u001b[0m                                                                                                               \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m \u001b[37mLLM Post-Training: A Deep Dive into Reasoning\u001b[0m                                                                   \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m \u001b[37mLarge Language Models\u001b[0m                                                                                           \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m \u001b[37mKomal Kumar∗, Tajamul Ashraf∗,...\u001b[0m                                                                               \u001b[33m│\u001b[0m\n",
       "\u001b[33m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">╭───────────────────────────────────────────────────── INFO ──────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Loading 31 pieces of context!</span>                                                                                   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33m╭─\u001b[0m\u001b[33m────────────────────────────────────────────────────\u001b[0m\u001b[33m INFO \u001b[0m\u001b[33m─────────────────────────────────────────────────────\u001b[0m\u001b[33m─╮\u001b[0m\n",
       "\u001b[33m│\u001b[0m \u001b[37mLoading 31 pieces of context!\u001b[0m                                                                                   \u001b[33m│\u001b[0m\n",
       "\u001b[33m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">╭───────────────────────────────────────────────────── INFO ──────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Split successful. Last chunk preview:</span>                                                                           <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">31</span>                                                                                                              <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">progressonscalableoversightforlargelanguagemodels,” arXiv</span>                                                       <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">preprint arXiv:2211.03540, 2022. 20</span>                                                                             <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">[34...</span>                                                                                                          <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33m╭─\u001b[0m\u001b[33m────────────────────────────────────────────────────\u001b[0m\u001b[33m INFO \u001b[0m\u001b[33m─────────────────────────────────────────────────────\u001b[0m\u001b[33m─╮\u001b[0m\n",
       "\u001b[33m│\u001b[0m \u001b[37mSplit successful. Last chunk preview:\u001b[0m                                                                           \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m \u001b[37m31\u001b[0m                                                                                                              \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m \u001b[37mprogressonscalableoversightforlargelanguagemodels,” arXiv\u001b[0m                                                       \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m \u001b[37mpreprint arXiv:2211.03540, 2022. 20\u001b[0m                                                                             \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m \u001b[37m[34...\u001b[0m                                                                                                          \u001b[33m│\u001b[0m\n",
       "\u001b[33m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llm_providers import GoogleAIModel\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from multi_agent import SyntheticDataGenerator\n",
    "from langchain.retrievers import BM25Retriever\n",
    "from utils import pdf_parser\n",
    "from tasks import Task\n",
    "\n",
    "load_dotenv()\n",
    "key = os.environ.get('GOOGLE_API_KEY')\n",
    "llm = GoogleAIModel(api_key=key)\n",
    "pdf_files = pdf_parser('./test_pdf/book.pdf')\n",
    "\n",
    "bm25retriever = BM25Retriever.from_documents(pdf_files, k=10)\n",
    "\n",
    "agent = SyntheticDataGenerator(llm=llm, \n",
    "                               retriever=bm25retriever,\n",
    "                               output_path='./output',\n",
    "                               buffer_size=10)\n",
    "\n",
    "example_task = Task(\n",
    "    task_name=\"dpo\",\n",
    "    localization='Lora Finetuning and Adapters',\n",
    "    task_description=\"I want to use your outputs to train a AI Researcher Model\",\n",
    "    rows_per_batch=12,\n",
    "    batch_size=15,\n",
    "    language=\"Vietnamese\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">╭──────────────────────────────────────────────── OUTPUT_MESSAGE ─────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">FORMATTED DOCUMENTS PREVIEW: </span>                                                                                   <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">- 13</span>                                                                                                            <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Model Category Source Description</span>                                                                               <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">1.Parameter-EfficientFine-Tuning&amp;ModelCompression</span>                                                               <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">LoRA[60] Low-Rank Adaptation Link Injects trainable low-rank adapters for efficient fine-tuning.</span>                <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">QLoRA[188] Qu</span>                                                                                                   <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m╭─\u001b[0m\u001b[32m───────────────────────────────────────────────\u001b[0m\u001b[32m OUTPUT_MESSAGE \u001b[0m\u001b[32m────────────────────────────────────────────────\u001b[0m\u001b[32m─╮\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37mFORMATTED DOCUMENTS PREVIEW: \u001b[0m                                                                                   \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37m- 13\u001b[0m                                                                                                            \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37mModel Category Source Description\u001b[0m                                                                               \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37m1.Parameter-EfficientFine-Tuning&ModelCompression\u001b[0m                                                               \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37mLoRA[60] Low-Rank Adaptation Link Injects trainable low-rank adapters for efficient fine-tuning.\u001b[0m                \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37mQLoRA[188] Qu\u001b[0m                                                                                                   \u001b[32m│\u001b[0m\n",
       "\u001b[32m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'retrieved_documents': ['\\n- 13\\nModel Category Source Description\\n1.Parameter-EfficientFine-Tuning&ModelCompression\\nLoRA[60] Low-Rank Adaptation Link Injects trainable low-rank adapters for efficient fine-tuning.\\nQLoRA[188] Quantized Adaptation Link Combines 4-bit quantization with LoRA to enable fine-tuning on consumer GPUs\\nGPTQ[189] Post-Training Quantization Link Optimal 4-bit quantization method for GPT-style models with minimal loss\\nSparseGPT[190] Pruning Link One-shot pruning that preserves model quality with compensation.\\nPEFT(HF) [191] Unified Fine-Tuning Link Library integrating LoRA, prefix tuning, and other parameter-efficient methods\\nBitsAndBytes[192] Low-Precision Training Link Enables 8-bit optimizers and 4-bit quantization for memory-efficient training\\nAdaLoRA[193] Adaptive Adaptation Link Dynamically allocates parameter budget between layers during fine-tuning\\nP-Tuningv2 [194] Prompt Optimization Link Learns continuous prompt embeddings through deep prompt tuning\\n2.DataManagement&Preprocessing\\nHFDatasets [195] Data Processing Link Unified API for 30k+ datasets with streaming, versioning, and preprocessing\\nWebDataset[196] Data Streaming Link Efficient tar-based sharding format for petascale distributed training\\nDVC[197] Data Versioning Link Git-like version control for datasets and machine learning pipelines\\nApacheArrow [198] Memory Format Link Language-agnostic columnar memory format for zero-copy data access\\nZstandard[199] Compression Link High-speed compression algorithm for training data storage/transfer\\nCleanlab[200] Data Quality Link Automatic detection of label errors and outliers in training datasets\\n3.DistributedTraining&Optimization\\nDeepSpeed[201] Training Optimization Link ZeRO parallelism, 3D parallelism, and memory optimizations for giant models\\nMegatron-LM[202] Model Parallelism Link NVIDIA’s optimized framework for large transformer model training\\nColossal-AI[203] Heterogeneous Training Link Unified system supporting multiple parallelization strategies\\nHorovod[204] Distributed Training Link MPI-inspired framework for multi-GPU/multi-node synchronization\\nRay[205] Distributed Computing Link Universal framework for distributed Python applications at scale\\n4.EfficientInference&Deployment\\nvLLM[206] Serving Optimization Link Paged attention implementation for high-throughput LLM serving\\nTensorRT[207] GPU Optimization Link NVIDIA’s inference optimizer with kernel fusion and quantization support\\nTriton[208] Serving Framework Link Production-grade serving with concurrent model execution support\\nONNX[209] Cross-Platform Link Unified inference engine with hardware-specific optimizations\\nOpenVINO[210] Intel Optimization Link Runtime for Intel CPUs/iGPUs with pruning/quantization support\\nXNNPACK[211] Mobile Inference Link Highly optimized floating-point kernels for ARM CPUs\\nGroq [212] AI Accelerator Link Deterministic low-latency inference via custom tensor streaming processor\\n5.IntegratedDevelopmentEcosystems\\nHFEcosystem [213] Full Stack Link Transformers + Datasets + Accelerate + Inference Endpoints\\nDeepSpeed[201] Training/Inference Link Microsoft’s end-to-end solution for billion-parameter models\\nPyTorch[214] Unified Framework Link Native LLM support via torch.compile and scaled dot-product attention\\nLLMReasoners [215] Advanced Reasoning Link Enhances LLM reasoning capabilities using advanced search algorithms.\\nTABLE 2: Comprehensive Overview of Modern LLM Methods and Frameworks.\\n4.6 Preference and Alignment SFT\\nWhile RLHF is not purely supervised, it starts with a su-\\npervised preferenceor alignment finetuning stage. This stage\\nuses human-labeled or human-ranked examples to teach the\\nmodel about desirable vs. undesirable outputs (e.g., safe vs.\\ntoxic). By training on these explicit preferences, the model\\nbecomes more aligned with user values, reducing harmful or\\noff-topic completions. Works like InstructGPT [58] illustrate\\nhowsupervisedpreferencedataiscriticalbeforerewardmodel\\ntraining andRL updates begin.\\n4.7 Efficient Finetuning\\nFully finetuning aLLM can be computationally and memory-\\nintensive, particularly as model sizes grow into the tens or\\nhundreds of billions of parameters. To address these chal-\\nlenges, parameter-efficient finetuning (PEFT) techniques intro-\\nduce a small set of trainable parameters or learnable prompts\\nwhile leaving most of the model weights frozen. Approaches\\nsuch as LoRA [60], Prefix Tuning [231], and Adapters [232]\\nexemplify this strategy by injecting lightweight modules (or\\nprompts) in specific layers, thus significantly reducing the\\nmemory footprint.\\nFigure 4 illustrates how these techniques fit into a broader\\necosystemthatinvolvessystem-leveloptimizations,dataman-\\nagement, and evaluation strategies for LLMs. In particular,\\nPEFT approaches can be combined with quantization and\\npruning methods [190, 188] to further minimize memory\\nusage and compute overhead, enabling finetuning on smaller\\nGPUs or even consumer-grade hardware. For instance,QLoRA\\nunifies 4-bit quantization with low-rank adaptation, while\\nBitsAndBytes provides 8-bit optimizers to makeLLM training\\nmore practical in constrained environments (Table 2).\\nMoreover, these PEFT methods still require supervised\\ndata to guide the adaptation process, but the reduction in\\nthe number of trainable parameters makes it more feasible\\nto use in-domain or task-specific datasets. This is especially\\nvaluable for specialized domains (e.g., medical or software\\ndevelopment), where data might be limited or expensive to\\nannotate. As shown in Table 2,PEFT (HF) integrates several\\nof these approaches (LoRA, prefix tuning, and more) into a\\nsingle library, streamlining deployment in both research and\\nproduction settings.\\nCombining efficient tuning designs likeLoRA\\nand QLoRA with system and data optimizations\\n(Figure4)enablescost-effectiveLLMadaptation\\nfor tasks like domain-specific text generation,\\nwithout expensive full fine-tuning.\\n5 Test-time Scaling Methods\\nWhileRL fine-tunes the model’s policy, test-time scaling (TTS)\\nenhances reasoning during inference typically without model', '\\n- 12\\nDataSystem\\nModel\\nAccelerators \\n(Groq, vLLM, Triton,\\netc.) \\nData Compression, Data\\nFiltering (TokenMerging,\\nRecapDataComp-18,\\netc.)\\nCo-Optimized Architectures\\n(FlashAttention, BlockSparse\\nIO, DeepSeek v3 etc.)\\nParallel Computing, \\nDistributed Training\\n(LoRA, PEFT,\\nDeepSpeed,etc.)\\nScaling law, Data mining\\n(Chinchilla, RETRO, C4\\ndata, etc.)\\nModel compression\\n(Bitsandbite, GPTQ,\\netc.)\\nEfficient Finetuning and Deployment\\nFig.4: ThisVenndiagramillustratestheinterplaybetweenSys-\\ntem, Data, and Model for efficient finetuning and deployment.\\nIt covers strategies like accelerators (Groq, vLLM), adaptation\\n(LoRA,PEFT),co-optimizedarchitectures(FlashAttention),data\\ncompression (TokenMerging), scaling laws (Chinchilla), and\\nmodelcompression( GPTQ)toboostperformanceandscalability.\\nperformance. It allows smaller models to inherit advanced\\nreasoning capabilities, making them competitive on challeng-\\ning benchmarks without the computational costs of full-scale\\nRL training. Finally,distillation plays a pivotal role: the top-\\nperforming model, DeepSeek-R1 [40], serves as a teacher to\\nsmaller architectures (e.g., Qwen or Llama families, ranging\\nfrom1.5Bto70Bparameters).Thistransferallowsthesmaller\\nmodels to inherit advanced reasoning capabilities, making\\nthem competitive on challenging benchmarks without incur-\\nring the computational costs of full-scaleRL training.\\nDistillation democratizes advanced reasoning\\ncapabilities, enabling smaller models to achieve\\ncompetitive performance with reduced compu-\\ntational overhead.\\n4 Supervised Finetuning in LLMs\\nAs shown in Figure 2, finetuning forms a basic component of\\nLLM post-training recipes. In this section, we summarize the\\ndifferent types ofLLM fine-tuning mechanisms.\\n4.1 Instruction finetuning\\nIn instruction finetuning, a model is trained on curated pairs\\nof instruction (prompt) and response (completion). The main\\ngoal is to guide theLLM to follow a user-provided instruction\\naccurately and helpfully, regardless of the task domain. This\\nusually involves compiling large, diverse instruction-response\\ndatasets covering many task types (e.g., summarization, QA,\\nclassification, creative writing). Models such as T0 [178],\\nFLAN [179], Alpaca [180], Vicuna [181] and Dolly [182]\\ndemonstrate how instruction-finetunedLLMs can outperform\\nbase models on zero-shot or few-shot tasks by virtue of their\\nenhanced instruction-following abilities.\\n4.2 Dialogue (Multi-turn) Finetuning\\nSome LLMs undergo dialogue-style finetuning to better handle\\nmulti-turn conversations. Different from instruction tuning\\ndescribed above, here the data takes the form of a contin-\\nuous dialogue (multi-turn conversations) instead of a single\\nprompt-response pair. In this approach, training data consists\\nof chat transcripts with muliple user queries and system re-\\nsponses, ensuring the model learns to maintain context across\\nturns and produce coherent replies. Models like LaMDA [183]\\nand ChatGPT [39] highlight how dialogue-tuned LLMs can\\nfeel more interactive and context-aware. While dialogue fine-\\ntuning can overlap with instruction finetuning (because many\\ninstructions come in a chat format), specialized conversation\\ndata often yields more natural, multi-turn user experiences.\\n4.3 CoT Reasoning finetuning\\nChain-of-Thought (CoT) reasoning finetuning teaches models\\nto produce step-by-step reasoning traces instead of just final\\nanswers. By exposing intermediate rationales or thoughts,\\nCoT finetuning can improve both interpretability and accu-\\nracy on complex tasks (e.g., math word problems, multi-\\nhop QA). In practice, CoT finetuning uses supervised rea-\\nsoning annotations (often handcrafted by experts) to show\\nhow a solution unfolds. Notable early work includes Chain-\\nof-Thought Prompting [8] and Self-Consistency [184], which\\ninitially applied the idea to prompting; subsequent efforts\\n(e.g., Chain-of-Thought Distillation [185]) adapt it to a full\\nfinetuning or student-teacher paradigm. These efforts have\\nalso been extended to the multimodal domain, e.g., LlaVA-\\nCoT [186] and LlamaV-o1 [187] where image, QA and CoT\\nreasoning steps are used inLLM finetuning.\\n4.4 Domain-Specific (Specialized) Finetuning\\nWhen an LLM needs to excel in a specific domain (e.g.,\\nbiomedicine, finance, or legal), domain-specific finetuning is\\nused. Here, a curated corpus of domain-relevant text and la-\\nbeled examples is employed to finetune theLLM. For instance,\\nBioGPT [71] and BiMediX [216] specialize in biomedical\\nliterature, FinBERT [217] for financial texts, ClimatGPT\\n[218, 219] for climate and sustainability and CodeT5 [220]\\nfor code understanding. Supervised finetuning in these do-\\nmains often includes classification, retrieval, or QA tasks with\\ndomain-specific data, ensuring the model’s parameters adapt\\nto the specialized language and concepts of the field. Domain-\\nspecific finetuning is also extended to vision-language models\\nsuch as, [221] finetuned on remote sensing imagery, [222] on\\nmedical imaging modalities, [223, 224, 225] on spatiotemporal\\nvideo inputs, and [226] adapted for chart understanding.\\n4.5 Distillation-Based Finetuning\\nLarge ‘teacher’ models are sometimes used to produce labeled\\ndata or rationales, which a smaller ‘student’ model finetunes\\non, this is generally called knowledge distillation [227, 228].\\nIn the context ofLLMs, CoT Distillation [185] is one example\\nwhere a powerful teacher LLM generates intermediate rea-\\nsoning steps, and the studentLLM is finetuned to reproduce\\nboth the final answer and the reasoning chain. Step-by-step\\ndistillation [229] generates descriptive rationales alongside\\nfinal answers to train smaller models through distillation\\nwith smaller datasets. This approach can yield lighter, faster\\nmodels that retain much of the teacher’s performance, even in\\nzero-shot or few-shot tasks [230].', '\\n- 1\\nLLM Post-Training: A Deep Dive into Reasoning\\nLarge Language Models\\nKomal Kumar∗, Tajamul Ashraf∗, Omkar Thawakar, Rao Muhammad Anwer, Hisham Cholakkal,\\nMubarak Shah, Ming-Hsuan Yang, Phillip H.S. Torr, Fahad Shahbaz Khan, Salman Khan\\nAbstract—Large Language Models (LLMs) have transformed the natural language processing landscape and brought to life diverse\\napplications. Pretraining on vast web-scale data has laid the foundation for these models, yet the research community is now\\nincreasingly shifting focus toward post-training techniques to achieve further breakthroughs. While pretraining provides a broad\\nlinguistic foundation, post-training methods enableLLMs to refine their knowledge, improve reasoning, enhance factual accuracy, and\\nalign more effectively with user intents and ethical considerations. Fine-tuning, reinforcement learning, and test-time scaling have\\nemerged as critical strategies for optimizingLLMs performance, ensuring robustness, and improving adaptability across various\\nreal-world tasks. This survey provides a systematic exploration of post-training methodologies, analyzing their role in refining LLMs\\nbeyond pretraining, addressing key challenges such as catastrophic forgetting, reward hacking, and inference-time trade-offs. We\\nhighlight emerging directions in model alignment, scalable adaptation, and inference-time reasoning, and outline future research\\ndirections. We also provide a public repository to continually track developments in this fast-evolving field:\\nhttps://github.com/mbzuai-oryx/Awesome-LLM-Post-training.\\nIndex Terms—Reasoning Models, Large Language Models, Reinforcement Learning, Reward Modeling, Test-time Scaling\\n✦\\n1 Introduction\\nC\\nontemporary Large Language Models (LLMs) exhibit\\nremarkable capabilities across a vast spectrum of tasks,\\nencompassing not only text generation [1, 2, 3] and question-\\nanswering [4, 5, 6, 7], but also sophisticated multi-step rea-\\nsoning [8, 9, 10, 11]. They power applications in natural\\nlanguage understanding [12, 13, 14, 15, 16, 17], content gener-\\nation [18, 19, 20, 21, 22, 23, 24, 25], automated reasoning [26,\\n27, 28, 29], and multimodal interactions [30, 31, 32, 33]. By\\nleveraging vast self-supervised training corpora, these models\\noften approximate human-like cognition [34, 35, 36, 37, 38],\\ndemonstrating impressive adaptability in real-world settings.\\nDespite these impressive achievements,LLMs remain prone\\nto critical shortcomings. They can generate misleading or\\nfactually incorrect content (commonly referred to as “hal-\\nlucinations”) and may struggle to maintain logical consis-\\ntency throughout extended discourse [41, 42, 43, 44, 45, 46].\\nMoreover, the concept of reasoning inLLMs remains a topic\\nof debate. While these models can produce responses that\\nappear logically coherent, their reasoning is fundamentally\\ndistinct from human-like logical inference [47, 34, 48, 49].\\nThis distinction is crucial, as it helps explain whyLLMs can\\n• ∗Equal contribution. Corresponding authors (Email: ko-\\nmal.kumar@mbzuai.ac.ae, tajamul.ashraf@mbzuai.ac.ae)\\n• Komal Kumar, Tajamul Ashraf, Omkar Thawakar, Rao Muham-\\nmadAnwer,HishamCholakkal,SalmanKhanandFahadShahbaz\\nKhan are with Mohamed bin Zayed University of Artificial Intel-\\nligence, Abu Dhabi, UAE.\\n• MubarakShahiswiththeCenterforResearchinComputerVision\\nat the University of Central Florida, Orlando, FL 32816, USA.\\n• Ming-Hsuan Yang is with the University of California at Merced,\\nMerced, CA 95343 USA, and also with Google DeepMind, Moun-\\ntain View, CA 94043, USA.\\n• Philip H.S. Torr is with the Department of Engineering Science,\\nUniversity of Oxford, Oxford OX1 2JD, UK.\\nK\\nLLM\\nLLM\\nPost \\ntraining\\nGPT-O1, O3\\nTuning\\nReinforce\\nScale\\nPolicyRewardOffline Policy\\nSearchConfidence\\nReasoning\\nFull Model\\nParm. Efficient\\nAdapters\\nLow-Rank\\nPrompt \\nEnd-to-End\\nDPO\\nSelf-Critique\\nTree-of-Thoughts\\nBeam Search\\nBest-of-N SearchChain-of-Thought\\nConfidence Sampling Consistency Decoding\\nMonte Carlo Search\\nRL Optimization\\nGRPO\\nPPO\\nTRPO\\nREINFORCE\\nVanilla PG\\nRLHFRLAIF\\nBehavior Cloning\\nOffline Batch\\nLlaMA 3.2 \\nXAONE 3.0\\nDecoding\\nTraining\\nQwen\\nDeepSeek-R1\\nClaude 3.5 Sonnet\\nMistral Large 2\\nClaude2\\nQwen-32B-Preview\\nDeepSeek-R1\\nLlaMA 3.3 \\nGPT-3\\nLlaMA 3.3 \\nLlaMA 3.1 \\nKnowledge\\n          Distillation\\nStarling-7B\\nOREO\\nSearch Against Verifiers\\nDistilBERT\\nALBERT\\nMiniLM\\nGPT-4, 4O, O1\\nClaude3 Mistral Large 2\\nGemini 1.5\\nAlphaGo\\nQwen-32B-Preview\\nAlphaGo\\nGemini 1.5\\nLLM post-training alignment\\nAlgorithmic categorization\\nAlgorithms\\nLLMs\\n§ 3\\n§ 4\\n§ 5\\nFig. 1: A taxonomy of post-training approaches for LLMs\\n(LLMs), categorized into Fine-tuning, Reinforcement Learn-\\ning, and Test-time Scaling methods. We summarize the key\\ntechniques used in recentLLM models, such as GPT-4 [39],\\nLLaMA 3.3 [13], and Deepseek R1 [40].\\nproduce compelling outputs while still stumbling on relatively\\nsimple logical tasks. Unlike symbolic reasoning that manipu-\\nlates explicit rules and facts,LLMs operate in an implicit and\\nprobabilistic manner [50, 42, 51]. For the scope of this work,\\narXiv:2502.21321v1  [cs.CL]  28 Feb 2025', '\\n- 19\\nIn terms of use cases, TTS is useful for scenarios with\\nflexible inference budget or when base models already exhibit\\nreasonable competence in the task. Conversely, pretraining is\\nessential for tasks requiring fundamentally new capabilities\\n(e.g., reasoning on novel domains) where inference-time opti-\\nmizations alone may not suffice.\\nThere are notable tradeoffs between the two approaches.\\nTTS reduces upfront training costs, making it attractive for\\nflexible, on-the-go optimization, but requires dynamic com-\\npute allocation at inference. Pretraining, on the other hand,\\nincurshighinitialcostsbutguaranteesconsistentperformance\\nwithout additional runtime overhead, making it ideal for\\nlarge-scale API deployments or latency-sensitive applications.\\nOverall, TTS and pretraining are complementary in nature.\\nFuture LLM systems may adopt a hybrid approach, where\\nsmaller base models are pretrained with essential knowledge,\\nwhile TTS dynamically enhances responses through adaptive,\\non-demand computation. This synergy enables more cost-\\neffective and efficient large-scale model deployment.\\nChoose pretraining for foundational capabil-\\nities and test-time scaling for accurate context-\\naware refinement.\\n6 Benchmarks for LLM Post-training Evaluation\\nTo evaluate the success of LLM post-training phases, a di-\\nverse set of benchmarks have been proposed covering mul-\\ntiple domains: reasoning tasks, alignment, multilinguality,\\ngeneralcomprehension,anddialogueandsearchtasks.Awell-\\nstructured evaluation framework ensures a comprehensive\\nunderstanding of an LLM strengths, and limitations across\\nvarious tasks. These benchmarks play a crucial role inLLM\\npost-processingstages,wheremodelsundergofine-tuning,cal-\\nibration, alignment, and optimization to improve response ac-\\ncuracy, robustness, and ethical compliance. Next, we explain\\nthe main benchmark gorups. Table 3 provides an overview of\\nkey datasets categorized under these benchmark groups.\\nReasoningBenchmarks. These benchmarks assessLLMs on\\ntheir ability to perform logical, mathematical, and scientific\\nreasoning. Mathematical reasoning datasets like MATH [269],\\nGSM8K [270], and MetaMathQA [271] test models on\\nproblem-solving, multi-step arithmetic, and theorem-based\\nproblem formulations. Scientific and multimodal reasoning\\nbenchmarks such as WorldTree V2 [272] and MMMU [274]\\nevaluate knowledge in physics, chemistry, and multimodal\\nunderstanding, which are crucial for fact-checking and veri-\\nfication processes in LLM-generated responses. Additionally,\\ndatasets like PangeaBench [273] extend reasoning tasks into\\nmultilingual and cultural domains, enabling models to refine\\ncross-lingual reasoning. These benchmarks help determine\\nhow well models can process structured knowledge and apply\\nlogical deductions.\\nRL Alignment Benchmarks. RL alignment benchmarks\\nare central to LLM alignment and post-training optimiza-\\ntion. They refine response generation, ethical constraints, and\\nuser-aligned outputs through RLHF. Datasets such as Help-\\nSteer [280] and UltraFeedback [281] evaluate models based\\non multi-attribute scoring and alignment with user instruc-\\ntions. Anthropic’s HH-RLHF [121] explores how well mod-\\nTABLE 3:Comprehensive Overview of Reasoning, RL Align-\\nment, and Multilingual Datasets. Here, pointwise and pairwise\\nrefer to different methods of evaluating model performance\\nacross various tasks.\\nDatasets Domain Type#SamplesEvaluation Criteria\\nReasoning Benchmarks\\nMATH[269] Math Reasoning Pointwise 7,500 Step-by-step solutionsGSM8K[270] Math ReasoningPointwise8.5K Multi-step reasoningMetaMathQA[271] Math Reasoning Pointwise 40K+ Self-verification, FOBARWorldTree V2[272] Science QAPointwise1,680 Multi-hop explanationsPangeaBench[273] Multimodal Reasoning Pairwise 47 Langs. Cultural understandingMMMU[274] Science/MathPointwiseCollege-LevelPhysics, Chemistry, BilingualTruthfulQA[275] QA/Reasoning Pointwise N/A TruthfulnessMathInstruct[276] Math ReasoningPointwise262K CorrectnessMMLU[277, 278] Multitask ReasoningPointwise57 TasksBroad knowledge evaluationMMLU-Fairness[277] Fairness/Reasoning Pointwise N/A Bias/Equity AnalysisDROP[279] Reading/ReasoningPointwise96K Discrete reasoning over paragraphsBBH[175] Hard Reasoning Pairwise N/A Complex logical problem-solvingVRC-Bench[187] Multimodal Reasoning Pairwise N/A Visual Reasoning and Classification\\nRL Alignment Benchmarks\\nHelpSteer[280] RL Alignment Pairwise 37K+ Multi-attribute scoringAnthropic HH-RLHF[121] RL AlignmentPairwise42.5K Harmlessness alignmentUltraFeedback[281] RL Alignment Pairwise 64K Instruction-following, TruthfulnessD4RL[282] RL/ControlPointwiseN/A Offline RL across domainsMeta-World[283] RL/Control Pointwise N/A Multi-task robotic RLMineRL[284] RL/GamesPairwiseN/A Imitation learning, rewards\\nMultilingual Evaluation\\nCulturaX[285] Multilingual Pointwise 6.3T Deduplication, QualityPangeaIns[286] MultilingualPointwise6M Multilingual instructionsTydiQA[287] Multilingual Pointwise N/A Cross-lingual QAXGLUE[288] MultilingualPointwiseN/A Cross-lingual language tasksMM-Eval[289] Multilingual Pairwise 4,981 Task-oriented multilingual QAALM-Bench[289] Multilingual QA Pointwise N/A Multilingual Evaluation\\nGeneral Comprehension Benchmarks\\nBigBench[290] General ComprehensionPointwise 200+ Tasks Broad multi-domain evaluationChatbot Arena[291] ComprehensionPairwise33K User preferenceMTBench[291] Comprehension Pairwise 3K Multi-turn conversationsRewardBench[167] ComprehensionPairwise2,998 User preference\\nGeneral Comprehension Benchmarks\\nConvAI2[292] Dialogue Pointwise N/A Engagingness, ConsistencyMultiWOZ[293] Dialogue PointwiseN/A Task success, CoherenceTrec DL21&22[294, 295] Search Pointwise 1,549/2,673 Relevance scoringBEIR[296] Search Pointwise18 DatasetsInformation retrieval\\nStory & Recommendation Benchmarks\\nHANNA[297] Story Pointwise 1,056 Relevance, Coherence, ComplexityStoryER[298] Story Pairwise100K User preference-based rankingPKU-SafeRLHF[299] Values Pairwise 83.4K Helpfulness, HarmlessnessCvalue[300] Values Pairwise145K Safety, ResponsibilityNaturalInst.[301, 302] Instruction Tuning Pointwise 1,600+ Instruction-following evaluation\\nels learn human preference optimization through reinforce-\\nment learning with human feedback. D4RL [282] and Meta-\\nWorld [283] focus on robotic control and offline RL, which\\nhave implications for autonomous model decision-making.\\nMineRL [284] extendsRL testing into complex environments\\nsuch as Minecraft-based interactions, useful for trainingLLMs\\nin adaptive decision-making settings.\\nMultilingual Evaluation.Multilingual benchmarks are es-\\nsential forLLM post-processing in cross-lingual generalization,\\ntranslation adaptation, and fine-tuning for low-resource lan-\\nguages. CulturaX [285] and PangeaIns [286] evaluate tok-\\nenization, translation, and instruction-following in over 150\\nlanguages, ensuring fairness and diversity in model outputs.\\nTydiQA [287] and MM-Eval [289] target bilingual and task-\\noriented multilingual evaluation, enabling improvements in\\nLLM fine-tuning. These datasets ensure thatLLMs are not just\\nEnglish-centric but optimized for multilingual adaptability.\\nGeneral Comprehension Benchmarks.General compre-\\nhensionbenchmarkscontributetomodelfine-tuning,response\\ncoherence, and preference optimization. Datasets such as\\nChatbotArena[291],MTBench[291],andRewardBench[167]\\ntest user preference modeling and conversational fluency,\\ncrucial for LLM response ranking and re-ranking methods.\\nBigBench [290] evaluates broad multi-domain comprehension,\\nwhile MMLU [277, 278] measures correctness and informa-\\ntiveness. These datasets help in refiningLLM fluency, factual\\ncorrectness, and open-ended response generation.\\nDialogue and Search Benchmarks.Dialogue and search\\nbenchmarks play a key role in optimizingLLM retrieval-based\\nresponses, multi-turn coherence, and information retrieval ac-\\ncuracy. Datasets such as ConvAI2 [292] and MultiWOZ [293]', '\\n- 30\\nP. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, J. Schul-\\nman, J. Hilton, F. Kelton, L. Miller, M. Simens, A. Askell,\\nP. Welinder, P. F. Christiano, J. Leike, and R. Lowe, “Training\\nlanguage models to follow instructions with human feedback,”\\nin Advances in Neural Information Processing Systems 35:\\nAnnual Conference on Neural Information Processing Systems\\n2022, NeurIPS 2022, New Orleans, LA, USA, November 28\\n- December 9, 2022 (S. Koyejo, S. Mohamed, A. Agarwal,\\nD. Belgrave, K. Cho, and A. Oh, eds.), 2022. 20\\n[304] W. Saunders, C. Yeh, J. Wu, S. Bills, L. Ouyang, J. Ward, and\\nJ. Leike, “Self-critiquing models for assisting human evalua-\\ntors,” arXiv preprint arXiv:2206.05802, 2022. 20\\n[305] J.Kaplan,S.McCandlish,T.Henighan,T.B.Brown,B.Chess,\\nR. Child, S. Gray, A. Radford, J. Wu, and D. Amodei,\\n“Scaling laws for neural language models,” arXiv preprint\\narXiv:2001.08361, 2020. 20\\n[306] S. Roy and D. Roth, “Solving general arithmetic word prob-\\nlems,” 2016. 20\\n[307] Y. Jinnai, T. Morimura, K. Ariu, and K. Abe, “Regularized\\nbest-of-n sampling to mitigate reward hacking for language\\nmodel alignment,”arXiv preprint arXiv:2404.01054, 2024. 20\\n[308] L. Chen, C. Zhu, D. Soselia, J. Chen, T. Zhou, T. Goldstein,\\nH. Huang, M. Shoeybi, and B. Catanzaro, “Odin: Disentangled\\nreward mitigates hacking in rlhf,”ArXiv, vol. abs/2402.07319,\\n2024. 20\\n[309] T. Liu, W. Xiong, J. Ren, L. Chen, J. Wu, R. Joshi, Y. Gao,\\nJ. Shen, Z. Qin, T. Yu, D. Sohn, A. Makarova, J. Liu, Y. Liu,\\nB. Piot, A. Ittycheriah, A. Kumar, and M. Saleh, “Rrm: Ro-\\nbust reward model training mitigates reward hacking,”ArXiv,\\nvol. abs/2409.13156, 2024. 20\\n[310] C. Wang, Z. Zhao, Y. Jiang, Z. Chen, C. Zhu, Y. Chen, J. Liu,\\nL. Zhang, X. Fan, H. Ma, and S.-Y. Wang, “Beyond reward\\nhacking: Causal rewards for large language model alignment,”\\n2025. 20\\n[311] C. B. Browne, E. Powley, D. Whitehouse, S. M. Lucas, P. I.\\nCowling, P. Rohlfshagen, S. Tavener, D. Perez, S. Samothrakis,\\nand S. Colton, “A survey of monte carlo tree search methods,”\\nIEEE Transactions on Computational Intelligence and AI in\\ngames, vol. 4, no. 1, pp. 1–43, 2012. 20\\n[312] A. Madaan, N. Tandon, P. Gupta, S. Hallinan, L. Gao,\\nS. Wiegreffe, U. Alon, N. Dziri, S. Prabhumoye, Y. Yang,et al.,\\n“Self-refine: Iterative refinement with self-feedback,”Advances\\nin Neural Information Processing Systems, vol. 36, 2024. 20\\n[313] Y. Fu, H. Peng, A. Sabharwal, P. Clark, and T. Khot,\\n“Complexity-based prompting for multi-step reasoning,” in\\nThe Eleventh International Conference on Learning Represen-\\ntations, 2022. 20\\n[314] B. Johnson, “Metacognition for artificial intelligence system\\nsafety–an approach to safe and desired behavior,”Safety Sci-\\nence, vol. 151, p. 105743, 2022. 20, 21\\n[315] OpenAI, “Early access for safety testing,” 2024. 20\\n[316] Y. Yan, X. Lou, J. Li, Y. Zhang, J. Xie, C. Yu, Y. Wang,\\nD. Yan, and Y. Shen, “Reward-robust rlhf in llms,”ArXiv,\\nvol. abs/2409.15360, 2024. 20\\n[317] W. Yu, Z. Sun, J. Xu, Z. Dong, X. Chen, H. Xu, and J.-\\nR. Wen, “Explainable legal case matching via inverse optimal\\ntransport-based rationale extraction,” in Proceedings of the\\n45th International ACM SIGIR Conference on Research and\\nDevelopment in Information Retrieval, pp. 657–668, 2022. 20\\n[318] A. Amini, S. Gabriel, P. Lin, R. Koncel-Kedziorski, Y. Choi,\\nand H. Hajishirzi, “Mathqa: Towards interpretable math word\\nproblem solving with operation-based formalisms,” 2019. 20\\n[319] L. Chen, O. Sinavski, J. Hünermann, A. Karnsund, A. J. Will-\\nmott, D. Birch, D. Maund, and J. Shotton, “Driving with llms:\\nFusingobject-levelvectormodalityforexplainableautonomous\\ndriving,”2024IEEEInternationalConferenceon Roboticsand\\nAutomation (ICRA), pp. 14093–14100, 2023. 20\\n[320] R. Poulain, H. Fayyaz, and R. Beheshti, “Bias patterns in the\\napplication of llms for clinical decision support: A comprehen-\\nsive study,”arXiv preprint arXiv:2404.15149, 2024. 20\\n[321] Z. Fan, R. Chen, R. Xu, and Z. Liu, “Biasalert: A plug-and-\\nplay tool for social bias detection in llms,” arXiv preprint\\narXiv:2407.10241, 2024. 20\\n[322] M. Li, T. Shi, C. Ziems, M.-Y. Kan, N. F. Chen, Z. Liu, and\\nD. Yang, “Coannotating: Uncertainty-guided work allocation\\nbetween human and large language models for data annota-\\ntion,” arXiv preprint arXiv:2310.15638, 2023. 20\\n[323] A. Elangovan, J. Ko, L. Xu, M. Elyasi, L. Liu, S. Bodapati,\\nand D. Roth, “Beyond correlation: The impact of human un-\\ncertaintyinmeasuringtheeffectivenessofautomaticevaluation\\nandllm-as-a-judge,” arXivpreprintarXiv:2410.03775 ,2024. 20\\n[324] Y. R. Dong, T. Hu, and N. Collier, “Can llm be a personalized\\njudge?,” arXiv preprint arXiv:2406.11657, 2024. 20\\n[325] D. Wang, K. Yang, H. Zhu, X. Yang, A. Cohen, L. Li,\\nand Y. Tian, “Learning personalized story evaluation,”arXiv\\npreprint arXiv:2310.03304, 2023. 20\\n[326] H. Du, S. Liu, L. Zheng, Y. Cao, A. Nakamura, and L. Chen,\\n“Privacy in fine-tuning large language models: Attacks, de-\\nfenses, and future directions,” 2024. 20, 22\\n[327] L. Yuan, W. Li, H. Chen, G. Cui, N. Ding, K. Zhang, B. Zhou,\\nZ. Liu, and H. Peng, “Free process rewards without process\\nlabels,” arXiv preprint arXiv:2412.01981, 2024. 20\\n[328] Y. J. Ma, W. Liang, G. Wang, D.-A. Huang, O. Bastani,\\nD. Jayaraman, Y. Zhu, L. Fan, and A. Anandkumar, “Eureka:\\nHuman-level reward design via coding large language models,”\\nArXiv, vol. abs/2310.12931, 2023. 20\\n[329] A. Havrilla, S. C. Raparthy, C. Nalmpantis, J. Dwivedi-Yu,\\nM. Zhuravinskyi, E. Hambro, and R. Railneau, “Glore: When,\\nwhere, and how to improve llm reasoning via global and local\\nrefinements,”ArXiv, vol. abs/2402.10963, 2024. 20\\n[330] M. Fawi, “Curlora: Stable llm continual fine-tuning and catas-\\ntrophic forgetting mitigation,” 2024. 20\\n[331] C. Fu, P. Chen, Y. Shen, Y. Qin, M. Zhang, X. Lin, Z. Qiu,\\nW. Lin, J. Yang, X. Zheng, K. Li, X. Sun, and R. Ji, “Mme:\\nA comprehensive evaluation benchmark for multimodal large\\nlanguage models,”ArXiv, vol. abs/2306.13394, 2023. 20\\n[332] Y. Wang, Z. Yu, Z. Zeng, L. Yang, C. Wang, H. Chen, C. Jiang,\\nR. Xie, J. Wang, X. Xie, W. Ye, S.-B. Zhang, and Y. Zhang,\\n“Pandalm:Anautomaticevaluationbenchmarkforllminstruc-\\ntiontuningoptimization,” ArXiv,vol.abs/2306.05087,2023. 20\\n[333] Y. Sun, Z. Li, Y. Li, and B. Ding, “Improving lora in privacy-\\npreserving federated learning,” ArXiv, vol. abs/2403.12313,\\n2024. 20\\n[334] Y. He, Y. Kang, L. Fan, and Q. Yang, “Fedeval-llm: Federated\\nevaluation of large language models on downstream tasks with\\ncollective wisdom,”arXiv preprint arXiv:2404.12273, 2024. 20\\n[335] J. Park, S. Jwa, M. Ren, D. Kim, and S. Choi, “Offsetbias:\\nLeveragingdebiaseddatafortuningevaluators,” arXivpreprint\\narXiv:2407.06551, 2024. 20\\n[336] P. Lu, L. Qiu, K.-W. Chang, Y. N. Wu, S.-C. Zhu, T. Rajpuro-\\nhit, P. Clark, and A. Kalyan, “Dynamic prompt learning via\\npolicy gradient for semi-structured mathematical reasoning,”\\n2023. 20\\n[337] L. Zhang, A. Hosseini, H. Bansal, M. Kazemi, A. Kumar,\\nand R. Agarwal, “Generative verifiers: Reward modeling as\\nnext-token prediction,” inThe 4th Workshop on Mathematical\\nReasoning and AI at NeurIPS’24, 2024. 20\\n[338] S. Yang and D. Song, “FPC: Fine-tuning with prompt cur-\\nriculum for relation extraction,” in Proceedings of the 2nd\\nConference of the Asia-Pacific Chapter of the Association for\\nComputational Linguistics and the 12th International Joint\\nConference on Natural Language Processing (Volume 1: Long\\nPapers) (Y. He, H. Ji, S. Li, Y. Liu, and C.-H. Chang, eds.),\\n(Online only), pp. 1065–1077, Association for Computational\\nLinguistics, Nov. 2022. 20\\n[339] Y.Yu,W.Ping,Z.Liu,B.Wang,J.You,C.Zhang,M.Shoeybi,\\nand B. Catanzaro, “Rankrag: Unifying context ranking with\\nretrieval-augmented generation in llms,”Advances in Neural\\nInformation Processing Systems, vol. 37, pp. 121156–121184,\\n2025. 20\\n[340] X. V. Lin, X. Chen, M. Chen, W. Shi, M. Lomeli, R. James,\\nP. Rodriguez, J. Kahn, G. Szilvasy, M. Lewis,et al., “Ra-dit:\\nRetrieval-augmented dual instruction tuning,” inThe Twelfth', '\\n- 31\\nprogressonscalableoversightforlargelanguagemodels,” arXiv\\npreprint arXiv:2211.03540, 2022. 20\\n[344] N. Hollmann, S. Müller, and F. Hutter, “Llms for semi-\\nautomated data science: Introducing caafe for context-aware\\nautomated feature engineering,”CoRR, 2023. 20\\n[345] S. R. Motwani, C. Smith, R. J. Das, M. Rybchuk, P. H. Torr,\\nI. Laptev, F. Pizzati, R. Clark, and C. S. de Witt, “Malt:\\nImproving reasoning with multi-agent llm training,” arXiv\\npreprint arXiv:2412.01928, 2024. 21\\n[346] A. Estornell, J.-F. Ton, Y. Yao, and Y. Liu, “Acc-debate: An\\nactor-critic approach to multi-agent debate,”arXiv preprint\\narXiv:2411.00053, 2024. 21\\n[347] L. Luo, Y. Liu, R. Liu, S. Phatale, H. Lara, Y. Li, L. Shu,\\nY. Zhu, L. Meng, J. Sun,et al., “Improve mathematical rea-\\nsoning in language models by automated process supervision,”\\narXiv preprint arXiv:2406.06592, 2024. 21\\n[348] W. Shen, X. Zhang, Y. Yao, R. Zheng, H. Guo, and Y. Liu,\\n“Improving reinforcement learning from human feedback using\\ncontrastive rewards,”arXiv preprint arXiv:2403.07708, 2024.\\n21\\n[349] M. Ma, P. D’Oro, Y. Bengio, and P.-L. Bacon, “Long-term\\ncredit assignment via model-based temporal shortcuts,” in\\nDeep RL Workshop NeurIPS 2021, 2021. 21\\n[350] E. Pignatelli, J. Ferret, M. Geist, T. Mesnard, H. van Has-\\nselt, O. Pietquin, and L. Toni, “A survey of temporal credit\\nassignment in deep reinforcement learning,” arXiv preprint\\narXiv:2312.01072, 2023. 21\\n[351] H. Zhang and Y. Guo, “Generalization of reinforcement learn-\\ningwithpolicy-awareadversarialdataaugmentation,”2021. 21\\n[352] A. Ahmadian, C. Cremer, M. Gallé, M. Fadaee, J. Kreutzer,\\nO. Pietquin, A. Üstün, and S. Hooker, “Back to basics: Re-\\nvisiting reinforce style optimization for learning from human\\nfeedback in llms,”arXiv preprint arXiv:2402.14740, 2024. 21\\n[353] S. Lee, G. Lee, J. W. Kim, J. Shin, and M.-K. Lee, “Hetal: Ef-\\nficient privacy-preserving transfer learning with homomorphic\\nencryption,” 2024. 22\\n[354] Y.Wei,J.Jia,Y.Wu,C.Hu,C.Dong,Z.Liu,X.Chen,Y.Peng,\\nand S. Wang, “Distributed differential privacy via shuffling\\nversus aggregation: A curious study,”IEEE Transactions on\\nInformation Forensics and Security, vol. 19, pp. 2501–2516,\\n2024. 22\\n[355] W.Zhang,K.Tang,H.Wu,M.Wang,Y.Shen,G.Hou,Z.Tan,\\nP. Li, Y. Zhuang, and W. Lu, “Agent-pro: Learning to evolve\\nvia policy-level reflection and optimization,” arXiv preprint\\narXiv:2402.17574, 2024. 22\\n[356] C. Ma, J. Zhang, Z. Zhu, C. Yang, Y. Yang, Y. Jin,\\nZ. Lan, L. Kong, and J. He, “Agentboard: An analytical\\nevaluation board of multi-turn llm agents,” arXiv preprint\\narXiv:2401.13178, 2024. 22\\n[357] J.Zhang,J.Xiang,Z.Yu,F.Teng,X.Chen,J.Chen,M.Zhuge,\\nX.Cheng,S.Hong,J.Wang, etal.,“Aflow:Automatingagentic\\nworkflow generation,”arXiv preprint arXiv:2410.10762, 2024.\\n22\\n[358] H.D.Le,X.Xia,andZ.Chen,“Multi-agentcausaldiscoveryus-\\ning large language models,”arXiv preprint arXiv:2407.15073,\\n2024. 22\\n[359] D. M. Owens, R. A. Rossi, S. Kim, T. Yu, F. Dernon-\\ncourt, X. Chen, R. Zhang, J. Gu, H. Deilamsalehy, and\\nN. Lipka, “A multi-llm debiasing framework,”arXiv preprint\\narXiv:2409.13884, 2024. 22\\n[360] H. Zou, Q. Zhao, L. Bariah, Y. Tian, M. Bennis, S. Lasaulce,\\nM. Debbah, and F. Bader, “Genainet: Enabling wireless collec-\\ntive intelligence via knowledge transfer and reasoning,”ArXiv,\\nvol. abs/2402.16631, 2024. 22\\n[361] R. Lee, O. J. Mengshoel, A. Saksena, R. Gardner, D. Genin,\\nJ. Silbermann, M. Owen, and M. J. Kochenderfer, “Adaptive\\nstress testing: Finding likely failure events with reinforcement\\nlearning,” 2020. 22\\n[362] A. G. Baydin, B. A. Pearlmutter, D. Syme, F. Wood, and\\nP. Torr, “Gradients without backpropagation,” 2022. 22\\n[363] Y. Liu, C. Cai, X. Zhang, X. Yuan, and C. Wang, “Arondight:\\nRed teaming large vision language models with auto-generated\\nmulti-modal jailbreak prompts,” inACM Multimedia, 2024. 22\\n[364] D. Kim, K. Lee, J. Shin, and J. Kim, “Aligning large language\\nmodels with self-generated preference data,” arXiv preprint\\narXiv:2406.04412, 2024. 22\\n[365] S. Ebrahimi, S. Ö. Arik, T. Nama, and T. Pfister, “Crome:\\nCross-modal adapters for efficient multimodal llm,” ArXiv,\\nvol. abs/2408.06610, 2024. 22\\n[366] H. Xia, Y. Li, C. T. Leong, W. Wang, and W. Li, “Tokenskip:\\nControllable chain-of-thought compression in llms,” 2025. 22\\n[367] Z. Ma, W. Wu, Z. Zheng, Y. Guo, Q. Chen, S. Zhang, and\\nX. Chen, “Leveraging speech ptm, text llm, and emotional tts\\nfor speech emotion recognition,”ICASSP 2024 - 2024 IEEE\\nInternational Conference on Acoustics, Speech and Signal Pro-\\ncessing (ICASSP), pp. 11146–11150, 2023. 22\\n[368] Z. Xi, W. Chen, B. Hong, S. Jin, R. Zheng, W. He, Y. Ding,\\nS. Liu, X. Guo, J. Wang, H. Guo, W. Shen, X. Fan, Y. Zhou,\\nS. Dou, X. Wang, X. Zhang, P. Sun, T. Gui, Q. Zhang,\\nand X. Huang, “Training large language models for reasoning\\nthrough reverse curriculum reinforcement learning,” ArXiv,\\nvol. abs/2402.05808, 2024. 22\\n[369] O. Y. Lee, A. Xie, K. Fang, K. Pertsch, and C. Finn,\\n“Affordance-guided reinforcement learning via visual prompt-\\ning,” ArXiv, vol. abs/2407.10341, 2024. 22\\n[370] H. Xu, Z. Zhu, D. Ma, S. Zhang, S. Fan, L. Chen, and K. Yu,\\n“Rejection improves reliability: Training llms to refuse un-\\nknown questions using rl from knowledge feedback,”ArXiv,\\nvol. abs/2403.18349, 2024. 22\\n[371] X.Chen,J.Xu,T.Liang,Z.He,J.Pang,D.Yu,L.Song,Q.Liu,\\nM. Zhou, Z. Zhang, R. Wang, Z. Tu, H. Mi, and D. Yu, “Do not\\nthinkthatmuchfor2+3=?ontheoverthinkingofo1-likellms,”\\nArXiv, vol. abs/2412.21187, 2024. 22\\n[372] M. Kemmerling, D. Lütticke, and R. H. Schmitt, “Beyond\\ngames: a systematic review of neural monte carlo tree search\\napplications,” Applied Intelligence, vol. 54, no. 1, pp. 1020–\\n1046, 2024. 22\\n[373] Y. Li, H. Wen, W. Wang, X. Li, Y. Yuan, G. Liu, J. Liu,\\nW.Xu,X.Wang,Y.Sun,R.Kong,Y.Wang,H.Geng,J.Luan,\\nX. Jin, Z.-L. Ye, G. Xiong, F. Zhang, X. Li, M. Xu, Z. Li, P. Li,\\nY. Liu, Y. Zhang, and Y. Liu, “Personal llm agents: Insights\\nandsurveyaboutthecapability,efficiencyandsecurity,” ArXiv,\\nvol. abs/2401.05459, 2024. 22\\n[374] H. Li, L. Ding, M. Fang, and D. Tao, “Revisiting catastrophic\\nforgetting in large language model tuning,” 2024. 22\\n[375] N. Alzahrani, H. A. Alyahya, Y. Alnumay, S. Alrashed, S. Al-\\nsubaie, Y. Almushaykeh, F. Mirza, N. Alotaibi, N. Altwairesh,\\nA. Alowisheq, M. S. Bari, and H. Khan, “When benchmarks\\nare targets: Revealing the sensitivity of large language model\\nleaderboards,” 2024. 22', '\\n- 2\\n‘reasoning’ inLLMs refers to their ability to generate logically\\ncoherent responses based on statistical patterns in data rather\\nthan explicit logical inference or symbolic manipulation. Ad-\\nditionally, models trained purely via next-token prediction\\ncan fail to align with user expectations or ethical standards,\\nespecially in ambiguous or malicious scenarios [4, 52]. These\\nissues underscore the need for specialized strategies that ad-\\ndress reliability, bias, and context sensitivity inLLM outputs.\\nLLMs training can be broadly categorized into two stages:\\npre-training, which generally relies on a next-token prediction\\nobjective over large-scale corpora, and post-training, encom-\\npassing multiple rounds of fine-tuning and alignment. Post-\\ntraining mechanisms aim to mitigate LLMs limitations by\\nrefining model behavior and aligning outputs with human\\nintent, mitigating biases or inaccuracies [53].\\nAdapting LLMs to domain-specific tasks often involves\\ntechniques likefine-tuning [54, 55, 56], which enables task-\\nspecific learning but risks overfitting and incurs high com-\\nputational costs. To address these challenges, approaches\\nsuch asReinforcement Learning (RL) [57, 58, 59] enhance\\nadaptability by leveraging dynamic feedback and optimizing\\nsequential decision-making. Additionally, advances inscal-\\ning techniques, including Low-Rank Adaptation (LoRA) [60],\\nadapters, and Retrieval-Augmented Generation (RAG) [61,\\n62, 63], improve both computational efficiency and factual\\naccuracy. These strategies, coupled with distributed train-\\ning frameworks, facilitate large-scale deployment and further\\nboost the usability ofLLMs across diverse applications (Fig-\\nure 1). Through these targeted post-training interventions,\\nLLMs become better aligned with human intent and ethical\\nrequirements, ultimately enhancing their real-world applica-\\nbility. Below, we summarize key post-training stages.\\na) Fine-Tuning in LLMs: Fine-tuning adapts pre-trained\\nLLMs to specific tasks or domains by updating parameters on\\ncurated datasets [64, 65, 66, 54, 55, 67, 56]. WhileLLMs gen-\\neralize well after large-scale pretraining, fine-tuning enhances\\nperformance in tasks like sentiment analysis [68, 69], question\\nanswering, and domain-specific applications such as medical\\ndiagnosis [70, 71, 72]. This process, typically supervised,\\naligns models with task requirements but poses challenges like\\noverfitting, high computational costs, and sensitivity to data\\nbiases [56, 31, 16]. To this end, parameter-efficient techniques\\nlike LoRA [60] and adapters learn task-specific adaptation by\\nupdating explicit parameters, significantly reducing compu-\\ntational overhead. As models specialize, they may struggle\\nwith out-of-domain generalization, underscoring the trade-off\\nbetween specificity and versatility.\\nFine-tuning tailors LLMs for specific tasks,\\nimproving performance but risking overfitting,\\nhigh compute costs, and reduced generalization.\\nb) Reinforcement Learning inLLMs: In conventionalRL,\\nan agent interacts with a structured environment, taking\\ndiscrete actions to transition between states while maximiz-\\ning cumulative rewards [73].RL domains—such as robotics,\\nboardgames,andcontrolsystems—featurewell-definedstate-\\naction spaces and clear objectives [74, 75].RL in LLMs differs\\nsignificantly. Instead of a finite action set,LLMs select tokens\\nfrom a vast vocabulary, and their evolving state comprises an\\never-growing text sequence [16, 59, 76, 57]. This complicates\\nplanning and credit assignment, as the impact of token se-\\nlection may only emerge later. Feedback in language-based\\nRL is also sparse [77], subjective, and delayed, relying on\\nheuristic evaluations and user preferences rather than clear\\nperformance metrics [78, 79, 58]. Additionally, LLMs must\\nbalance multiple, sometimes conflicting, objectives, unlike\\nconventional RL, which typically optimizes for a single goal.\\nHybrid approaches combining process-based rewards (e.g.,\\nchain-of-thought reasoning) with outcome-based evaluations\\n(e.g., response quality) help refine learning [8, 80, 81]. Thus,\\nRL for LLMs requires specialized optimization techniques to\\nhandle high-dimensional outputs, non-stationary objectives,\\nand complex reward structures, ensuring responses remain\\ncontextually relevant and aligned with user expectations.\\nReinforcement in LLMs extends beyond con-\\nventional RL as it navigates vast action spaces,\\nhandlessubjectiveanddelayedrewards,andbal-\\nances multiple objectives, necessitating special-\\nized optimization techniques.\\nc) Scaling in LLMs: Scaling is crucial for enhancing the\\nperformance and efficiency ofLLMs. It helps improve general-\\nization across tasks but introduces significant computational\\nchallenges [82, 83]. Balancing performance and resource ef-\\nficiency requires targeted strategies at inference. Techniques\\nlike CoT [8] reasoning and Tree-of-Thought (ToT) [84] frame-\\nworks enhance multi-step reasoning by breaking down com-\\nplex problems into sequential or tree-structured steps. Addi-\\ntionally, search-based techniques[85, 86, 87, 88] enable itera-\\ntive exploration of possible outputs, helping refine responses\\nand ensure higher factual accuracy. These approaches, com-\\nbined with methods like LoRA [60], adapters, and RAG [61,\\n62, 89], optimize the model’s ability to handle complex,\\ndomain-specific tasks at scale. RAG enhances factual accu-\\nracybydynamicallyretrievingexternalknowledge,mitigating\\nlimitations of static training data [62, 24, 90]. Distributed\\ntraining frameworks leverage parallel processing to manage\\nthe high computational demands of large-scale models. Test-\\ntime scaling optimizes inference by adjusting parameters dy-\\nnamically based on task complexity [83, 91]. Modifying depth,\\nwidth, or active layers balances computational efficiency and\\noutput quality, making it valuable in resource-limited or\\nvariable conditions. Despite advancements, scaling presents\\nchallenges such as diminishing returns, longer inference times,\\nand environmental impact, especially when search techniques\\nare performed at test time rather than during training [82].\\nEnsuring accessibility and feasibility is essential to maintain\\nhigh-quality, efficientLLM deployment.\\nTest-time scaling enhances the adaptability\\nof LLMs by dynamically adjusting computational\\nresources during inference.\\n1.1 Prior Surveys\\nRecent surveys on RL and LLMs provide valuable insights\\nbut often focus on specific aspects, leaving key post-training', '\\n- 29\\n[267] W. Merrill and A. Sabharwal, “The expressive power\\nof transformers with chain of thought,” arXiv preprint\\narXiv:2310.07923, 2023. 18\\n[268] C. V. Snell, J. Lee, K. Xu, and A. Kumar, “Scaling test-\\ntime compute optimally can be more effective than scaling llm\\nparameters,” in The Thirteenth International Conference on\\nLearning Representations. 18\\n[269] Saxton et al., “Analysing mathematical reasoning abilities of\\nneural models,”arXiv:1904.01557, 2019. 19\\n[270] K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun,\\nL. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano,\\nC. Hesse, and J. Schulman, “Training verifiers to solve math\\nword problems,”arXiv preprint arXiv:2110.14168, 2021. 19\\n[271] L. Yu, W. Jiang, H. Shi, J. Yu, Z. Liu, Y. Zhang, J. T. Kwok,\\nZ. Li, A. Weller, and W. Liu, “Metamath: Bootstrap your\\nown mathematical questions for large language models,”arXiv\\npreprint arXiv:2309.12284, 2023. 19\\n[272] Z. Xie, S. Thiem, J. Martin, E. Wainwright, S. Marmorstein,\\nand P. Jansen, “WorldTree v2: A corpus of science-domain\\nstructured explanations and inference patterns supporting\\nmulti-hop inference,” inProceedings of the Twelfth Language\\nResources and Evaluation Conference(N. Calzolari, F. Béchet,\\nP. Blache, K. Choukri, C. Cieri, T. Declerck, S. Goggi, H. Isa-\\nhara, B. Maegaard, J. Mariani, H. Mazo, A. Moreno, J. Odijk,\\nand S. Piperidis, eds.), (Marseille, France), pp. 5456–5473,\\nEuropean Language Resources Association, May 2020. 19\\n[273] F. Liu, E. Bugliarello, E. M. Ponti, S. Reddy, N. Collier, and\\nD. Elliott, “Visually grounded reasoning across languages and\\ncultures,” inProceedings of the 2021 Conference on Empirical\\nMethods in Natural Language Processing, (Online and Punta\\nCana, Dominican Republic), pp. 10467–10485, Association for\\nComputational Linguistics, Nov. 2021. 19\\n[274] X. Yue, Y. Ni, K. Zhang, T. Zheng, R. Liu, G. Zhang,\\nS. Stevens, D. Jiang, W. Ren, Y. Sun, C. Wei, B. Yu, R. Yuan,\\nR. Sun, M. Yin, B. Zheng, Z. Yang, Y. Liu, W. Huang, H. Sun,\\nY. Su, and W. Chen, “Mmmu: A massive multi-discipline\\nmultimodalunderstandingandreasoningbenchmarkforexpert\\nagi,” inProceedings of CVPR, 2024. 19\\n[275] S. Lin, J. Hilton, and O. Evans, “Truthfulqa: Measur-\\ning how models mimic human falsehoods,” arXiv preprint\\narXiv:2109.07958, 2021. 19\\n[276] X. Yue et al., “Mammoth: Building math generalist mod-\\nels through hybrid instruction tuning,” arXiv preprint\\narXiv:2309.05653, 2023. 19\\n[277] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika,\\nD. Song, and J. Steinhardt, “Measuring massive multitask lan-\\nguage understanding,” Proceedings of the International Con-\\nference on Learning Representations (ICLR), 2021. 19\\n[278] D. Hendrycks, C. Burns, S. Basart, A. Critch, J. Li, D. Song,\\nand J. Steinhardt, “Aligning ai with shared human values,”\\nProceedings of the International Conference on Learning Rep-\\nresentations (ICLR), 2021. 19\\n[279] D. Dua, Y. Wang, P. Dasigi, G. Stanovsky, S. Singh, and\\nM. Gardner, “DROP: A reading comprehension benchmark\\nrequiring discrete reasoning over paragraphs,” in Proc. of\\nNAACL, 2019. 19\\n[280] Z. Wang, Y. Dong, J. Zeng, V. Adams, M. N. Sreedhar,\\nD. Egert, O. Delalleau, J. P. Scowcroft, N. Kant, A. Swope,\\net al., “Helpsteer: Multi-attribute helpfulness dataset for\\nsteerlm,” arXiv preprint arXiv:2311.09528, 2023. 19\\n[281] G.Cui,L.Yuan,N.Ding,G.Yao,W.Zhu,Y.Ni,G.Xie,Z.Liu,\\nand M. Sun, “Ultrafeedback: Boosting language models with\\nhigh-quality feedback,” 2023. 19\\n[282] J. Fu, A. Kumar, O. Nachum, G. Tucker, and S. Levine, “D4rl:\\nDatasetsfordeepdata-drivenreinforcementlearning,”2020. 19\\n[283] T. Schmied, M. Hofmarcher, F. Paischer, R. Pascanu, and\\nS. Hochreiter, “Learning to modulate pre-trained models in rl,”\\nAdvances in Neural Information Processing Systems, vol. 36,\\n2024. 19\\n[284] W. H. Guss, B. Houghton, N. Topin, P. Wang, C. Codel,\\nM.Veloso,andR.Salakhutdinov,“Minerl:Alarge-scaledataset\\nof minecraft demonstrations,” 2019. 19\\n[285] T. Nguyen, C. V. Nguyen, V. D. Lai, H. Man, N. T. Ngo,\\nF. Dernoncourt, R. A. Rossi, and T. H. Nguyen, “CulturaX:\\nA cleaned, enormous, and multilingual dataset for large lan-\\nguage models in 167 languages,” in Proceedings of the 2024\\nJoint International Conference on Computational Linguistics,\\nLanguage Resources and Evaluation (LREC-COLING 2024)\\n(N. Calzolari, M.-Y. Kan, V. Hoste, A. Lenci, S. Sakti, and\\nN.Xue,eds.),(Torino,Italia),pp.4226–4237,ELRAandICCL,\\nMay 2024. 19\\n[286] X. Yue, Y. Song, A. Asai, S. Kim, J. de Dieu Nyandwi,\\nS. Khanuja, A. Kantharuban, L. Sutawika, S. Ramamoorthy,\\nand G. Neubig, “Pangea: A fully open multilingual multimodal\\nllm for 39 languages,”arXiv preprint arXiv:2410.16153, 2024.\\n19\\n[287] J. Devlin, M. Chang, K. Lee, and K. Toutanova, “BERT: pre-\\ntraining of deep bidirectional transformers for language under-\\nstanding,” CoRR, vol. abs/1810.04805, 2018. 19\\n[288] Y. Liang, N. Duan, Y. Gong, N. Wu, F. Guo, W. Qi, M. Gong,\\nL. Shou, D. Jiang, G. Cao, X. Fan, R. Zhang, R. Agrawal,\\nE. Cui, S. Wei, T. Bharti, Y. Qiao, J.-H. Chen, W. Wu, S. Liu,\\nF. Yang, D. Campos, R. Majumder, and M. Zhou, “Xglue: A\\nnew benchmark dataset for cross-lingual pre-training, under-\\nstanding and generation,”arXiv, vol. abs/2004.01401, 2020. 19\\n[289] G. Son, D. Yoon, J. Suk, J. Aula-Blasco, M. Aslan, V. T. Kim,\\nS. B. Islam, J. Prats-Cristià, L. Tormo-Bañuelos, and S. Kim,\\n“Mm-eval: A multilingual meta-evaluation benchmark for llm-\\nas-a-judge and reward models,” 2024. 19\\n[290] S. et.al, “Beyond the imitation game: Quantifying and extrap-\\nolating the capabilities of language models,” 2022. 19\\n[291] L. Zheng, W.-L. Chiang, Y. Sheng, S. Zhuang, Z. Wu,\\nY. Zhuang, Z. Lin, Z. Li, D. Li, E. Xing, et al., “Judging\\nllm-as-a-judge with mt-bench and chatbot arena,”Advances\\nin Neural Information Processing Systems, vol. 36, pp. 46595–\\n46623, 2023. 19\\n[292] E. Dinan, V. Logacheva, V. Malykh, A. H. Miller, K. Shuster,\\nJ. Urbanek, D. Kiela, A. Szlam, I. Serban, R. Lowe, S. Prab-\\nhumoye, A. W. Black, A. I. Rudnicky, J. Williams, J. Pineau,\\nM. S. Burtsev, and J. Weston, “The second conversational\\nintelligence challenge (convai2),”CoRR, vol. abs/1902.00098,\\n2019. 19\\n[293] M. Eric, R. Goel, S. Paul, A. Sethi, S. Agarwal, S. Gao,\\nand D. Hakkani-Tur, “MultiWOZ 2.1: A consolidated multi-\\ndomain dialogue dataset with state corrections and state track-\\ning baselines,” inProceedings of the 12th Language Resources\\nand Evaluation Conference, (Marseille, France), pp. 422–428,\\nEuropean Language Resources Association, May 2020. 19\\n[294] X. Li and D. Roth, “Learning question classifiers,” inCOLING\\n2002: The 19th International Conference on Computational\\nLinguistics, 2002. 19, 20\\n[295] E.Hovy,L.Gerber,U.Hermjakob,C.-Y.Lin,andD.Ravichan-\\ndran, “Toward semantics-based answer pinpointing,” inPro-\\nceedings of the First International Conference on Human Lan-\\nguage Technology Research, 2001. 19, 20\\n[296] N. Thakur, N. Reimers, A. Rücklé, A. Srivastava, and\\nI. Gurevych, “BEIR: A heterogeneous benchmark for zero-\\nshot evaluation of information retrieval models,” in Thirty-\\nfifth Conference on Neural Information Processing Systems\\nDatasets and Benchmarks Track (Round 2), 2021. 19, 20\\n[297] C.Chhun,F.M.Suchanek,andC.Clavel,“Dolanguagemodels\\nenjoy their own stories? Prompting large language models for\\nautomatic story evaluation,”Transactions of the Association\\nforComputationalLinguistics ,vol.12,pp. 1122–1142,2024. 19\\n[298] H.Chen,D.M.Vo,H.Takamura,Y.Miyao,andH.Nakayama,\\n“Storyer: Automatic story evaluation via ranking, rating and\\nreasoning,” 2022. 19\\n[299] J. Ji, M. Liu, J. Dai, X. Pan, C. Zhang, C. Bian, B. Chen,\\nR.Sun,Y.Wang,andY.Yang,“Beavertails:Towardsimproved\\nsafety alignment of llm via a human-preference dataset,”Ad-\\nvancesinNeuralInformationProcessingSystems ,vol.36,2024.\\n19, 20\\n[300] G. Xu, J. Liu, M. Yan, H. Xu, J. Si, Z. Zhou, P. Yi, X. Gao,\\nJ. Sang, R. Zhang,et al., “Cvalues: Measuring the values of\\nchinese large language models from safety to responsibility,”\\narXiv preprint arXiv:2307.09705, 2023. 19', '\\n- 27\\nIntroducing the world’s first truly open instruction-tuned llm,”\\n2023. 12\\n[183] R. Thoppilan, D. De Freitas, J. Hall, N. Shazeer, A. Kul-\\nshreshtha, H.-T. Cheng, A. Jin, T. Bos, L. Baker, Y. Du,\\netal.,“Lamda:Languagemodelsfordialogapplications,” arXiv\\npreprint arXiv:2201.08239, 2022. 12\\n[184] X. Wang, J. Wei, D. Schuurmans, Q. V. Le, E. H. Chi,\\nS. Narang, A. Chowdhery, and D. Zhou, “Self-consistency\\nimproves chain of thought reasoning in language models,” in\\nThe Eleventh International Conference on Learning Represen-\\ntations, 2023. 12, 15, 20\\n[185] L. C. Magister, J. Mallinson, J. Adamek, E. Malmi, and\\nA. Severyn, “Teaching small language models to reason,”arXiv\\npreprint arXiv:2212.08410, 2022. 12\\n[186] G. Xu, P. Jin, H. Li, Y. Song, L. Sun, and L. Yuan, “Llava-cot:\\nLet vision language models reason step-by-step,” 2024. 12\\n[187] O. Thawakar, D. Dissanayake, K. More, R. Thawkar, A. Heakl,\\nN. Ahsan, Y. Li, M. Zumri, J. Lahoud, R. M. Anwer,\\nH. Cholakkal, I. Laptev, M. Shah, F. S. Khan, and S. Khan,\\n“Llamav-o1: Rethinking step-by-step visual reasoning in llms,”\\n2025. 12, 19\\n[188] T. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer,\\n“Qlora: Efficient finetuning of quantized llms,”arXiv preprint\\narXiv:2305.14314, 2023. 13\\n[189] E. Frantar, S. Ashkboos, T. Hoefler, and D. Alistarh, “GPTQ:\\nAccurate post-training compression for generative pretrained\\ntransformers,” arXiv preprint arXiv:2210.17323, 2022. 13\\n[190] E. Frantar and D. Alistarh, “SparseGPT: Massive language\\nmodels can be accurately pruned in one-shot,”arXiv preprint\\narXiv:2301.00774, 2023. 13\\n[191] S. Mangrulkar, S. Gugger, L. Debut, Y. Belkada, S. Paul,\\nand B. Bossan, “Peft: State-of-the-art parameter-efficient fine-\\ntuning methods,” 2022. 13\\n[192] T. Dettmers, M. Lewis, Y. Belkada, and L. Zettlemoyer,\\n“Llm.int8(): 8-bit matrix multiplication for transformers at\\nscale,” arXiv preprint arXiv:2208.07339, 2022. 13\\n[193] Q. Zhang, M. Chen, A. Bukharin, P. He, Y. Cheng, W. Chen,\\nand T. Zhao, “Adaptive budget allocation for parameter-\\nefficientfine-tuning,”in TheEleventhInternationalConference\\non Learning Representations, 2023. 13, 20\\n[194] X. Liu, K. Ji, Y. Fu, Z. Du, Z. Yang, and J. Tang, “P-tuning v2:\\nPrompt tuning can be comparable to fine-tuning universally\\nacross scales and tasks,”CoRR, vol. abs/2110.07602, 2021. 13\\n[195] Q.Lhoest,A.VillanovadelMoral,Y.Jernite,A.Thakur,P.von\\nPlaten, S. Patil, J. Chaumond, M. Drame, J. Plu, L. Tunstall,\\nJ. Davison, M. Šaško, G. Chhablani, B. Malik, S. Brandeis,\\nT. Le Scao, V. Sanh, C. Xu, N. Patry, A. McMillan-Major,\\nP. Schmid, S. Gugger, C. Delangue, T. Matussière, L. Debut,\\nS. Bekman, P. Cistac, T. Goehringer, V. Mustar, F. Lagunas,\\nA.Rush,andT.Wolf,“Datasets:Acommunitylibraryfornatu-\\nrallanguageprocessing,”in Proceedingsofthe2021Conference\\non Empirical Methods in Natural Language Processing: System\\nDemonstrations, (Online and Punta Cana, Dominican Repub-\\nlic), pp. 175–184, Association for Computational Linguistics,\\nNov. 2021. 13\\n[196] A. Torralba and Others, “Webdataset: A format for petascale\\ndeeplearning.” Efficienttar-basedshardingformatforpetascale\\ndistributed training. 13\\n[197] I.Iterative,“Dvc:Dataversioncontrol.” Git-likeversioncontrol\\nfor datasets and machine learning pipelines. 13\\n[198] N. Richardson, I. Cook, N. Crane, D. Dunnington,\\nR. François, J. Keane, D. Moldovan-Grünfeld, J. Ooms,\\nJ. Wujciak-Jens, and Apache Arrow, arrow: Integration\\nto ’Apache’ ’Arrow’ , 2025. R package version 19.0.0,\\nhttps://arrow.apache.org/docs/r/. 13\\n[199] I. Facebook, “Zstandard: High-speed compression algorithm.”\\nHigh-speed compression algorithm for training data storage/-\\ntransfer. 13\\n[200] C. Team, “Cleanlab: The standard data-centric ai package for\\nmachine learning with noisy labels.” Automatic detection of\\nlabel errors and outliers in training datasets. 13\\n[201] R. Y. Aminabadi, S. Rajbhandari, M. Zhang, A. A. Awan,\\nC. Li, D. Li, E. Zheng, J. Rasley, S. Smith, O. Ruwase, and\\nY. He, “Deepspeed inference: Enabling efficient inference of\\ntransformer models at unprecedented scale,” 2022. 13\\n[202] M. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper, and\\nB. Catanzaro, “Megatron-lm: Training multi-billion parameter\\nlanguage models using model parallelism,” 2020. 13\\n[203] S. Li, H. Liu, Z. Bian, J. Fang, H. Huang, Y. Liu, B. Wang,\\nand Y. You, “Colossal-ai: A unified deep learning system for\\nlarge-scale parallel training,” 2023. 13\\n[204] A. Sergeev and M. D. Balso, “Horovod: fast and easy dis-\\ntributed deep learning in tensorflow,” 2018. 13\\n[205] P. Moritz, R. Nishihara, S. Wang, A. Tumanov, R. Liaw,\\nE. Liang, M. Elibol, Z. Yang, W. Paul, M. I. Jordan, and\\nI. Stoica, “Ray: A distributed framework for emerging ai ap-\\nplications,” 2018. 13\\n[206] W. Kwon, Z. Li, S. Zhuang, Y. Sheng, L. Zheng, C. H. Yu, J. E.\\nGonzalez, H. Zhang, and I. Stoica, “Efficient memory manage-\\nment for large language model serving with pagedattention,”\\n2023. 13\\n[207] Y. Zhou and K. Yang, “Exploring tensorrt to improve real-\\ntime inference for deep learning,” in2022 IEEE 24th Int Conf\\non High Performance Computing & Communications; 8th Int\\nConfonDataScience&Systems;20thIntConfonSmartCity;\\n8th Int Conf on Dependability in Sensor, Cloud & Big Data\\nSystems & Application (HPCC/DSS/SmartCity/DependSys),\\npp. 2011–2018, 2022. 13\\n[208] P.Tillet,H.-T.Kung,andD.Cox,“Triton:anintermediatelan-\\nguage and compiler for tiled neural network computations,” in\\nProceedingsofthe3rdACMSIGPLANInternationalWorkshop\\non Machine Learning and Programming Languages, pp. 10–19,\\n2019. 13\\n[209] O. Community, “Onnx: Open neural network exchange.” Uni-\\nfied inference engine with hardware-specific optimizations. 13\\n[210] I. Corporation, “Openvino: Intel optimization toolkit,” 2025.\\nRuntime for Intel CPUs/iGPUs with pruning/quantization\\nsupport. 13\\n[211] M. Dukhan, “The indirect convolution algorithm,” 2019. 13\\n[212] I. Groq, “Groq: Ai accelerator,” 2025. Deterministic low-\\nlatency inference via custom tensor streaming processor. 13\\n[213] J. Castaño, S. Martínez-Fernández, X. Franch, and J. Bogner,\\n“Analyzing the evolution and maintenance of ml models on\\nhugging face,” 2024. 13\\n[214] A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. De-\\nVito, Z. Lin, A. Desmaison, L. Antiga, and A. Lerer, “Auto-\\nmatic differentiation in pytorch,” inNIPS-W, 2017. 13\\n[215] S.Hao,Y.Gu,H.Luo,T.Liu,X.Shao,X.Wang,S.Xie,H.Ma,\\nA. Samavedhi, Q. Gao,et al., “Llm reasoners: New evaluation,\\nlibrary, and analysis of step-by-step reasoning with large lan-\\nguage models,”arXiv preprint arXiv:2404.05221, 2024. 13\\n[216] S. Pieri, S. S. Mullappilly, F. S. Khan, R. M. Anwer, S. Khan,\\nT. Baldwin, and H. Cholakkal, “Bimedix: Bilingual medical\\nmixture of experts llm,” arXiv preprint arXiv:2402.13253,\\n2024. 12\\n[217] Y. Yang, M. C. S. Uy, and A. Huang, “Finbert: A pretrained\\nlanguage model for financial communications,”arXiv preprint\\narXiv:2006.08097, 2020. 12\\n[218] D. Thulke, Y. Gao, P. Pelser, R. Brune, R. Jalota, F. Fok,\\nM. Ramos, I. van Wyk, A. Nasir, H. Goldstein,et al., “Cli-\\nmategpt: Towards ai synthesizing interdisciplinary research on\\nclimate change,”arXiv preprint arXiv:2401.09646, 2024. 12\\n[219] S.S.Mullappilly,A.Shaker,O.Thawakar,H.Cholakkal,R.M.\\nAnwer, S. Khan, and F. S. Khan, “Arabic mini-climategpt: A\\nclimate change and sustainability tailored arabic llm,”arXiv\\npreprint arXiv:2312.09366, 2023. 12\\n[220] Y. Wang, W. Wang, S. Joty, and S. C. Hoi, “Codet5: Identifier-\\naware unified pre-trained encoder-decoder models for code un-\\nderstandingandgeneration,” arXivpreprintarXiv:2109.00859 ,\\n2021. 12\\n[221] K. Kuckreja, M. S. Danish, M. Naseer, A. Das, S. Khan, and\\nF. S. Khan, “Geochat: Grounded large vision-language model\\nfor remote sensing,” inProceedings of the IEEE/CVF Confer-\\nence on Computer Vision and Pattern Recognition, pp. 27831–\\n27840, 2024. 12\\n[222] S. S. Mullappilly, M. I. Kurpath, S. Pieri, S. Y. Alseiari,\\nS. Cholakkal, K. Aldahmani, F. Khan, R. Anwer, S. Khan,\\nT. Baldwin, et al., “Bimedix2: Bio-medical expert lmm for', '\\n- 28\\n[224] B. Lin, Y. Ye, B. Zhu, J. Cui, M. Ning, P. Jin, and L. Yuan,\\n“Video-llava: Learning united visual representation by align-\\nment before projection,” arXiv preprint arXiv:2311.10122,\\n2023. 12\\n[225] H. Zhang, X. Li, and L. Bing, “Video-llama: An instruction-\\ntuned audio-visual language model for video understanding,”\\narXiv preprint arXiv:2306.02858, 2023. 12\\n[226] Y. Han, C. Zhang, X. Chen, X. Yang, Z. Wang, G. Yu, B. Fu,\\nand H. Zhang, “Chartllama: A multimodal llm for chart un-\\nderstandingandgeneration,” arXivpreprintarXiv:2311.16483 ,\\n2023. 12\\n[227] X.Zhu,J.Li,Y.Liu,C.Ma,andW.Wang,“Asurveyonmodel\\ncompression for large language models,”Transactions of the\\nAssociation for Computational Linguistics, vol. 12, pp. 1556–\\n1577, 2024. 12\\n[228] Z. Wan, X. Wang, C. Liu, S. Alam, Y. Zheng, J. Liu, Z. Qu,\\nS. Yan, Y. Zhu, Q. Zhang, et al., “Efficient large language\\nmodels: A survey,”arXiv preprint arXiv:2312.03863, 2023. 12\\n[229] C.-Y. Hsieh, C.-L. Li, C.-K. Yeh, H. Nakhost, Y. Fujii,\\nA. Ratner, R. Krishna, C.-Y. Lee, and T. Pfister, “Distill-\\ning step-by-step! outperforming larger language models with\\nless training data and smaller model sizes,” arXiv preprint\\narXiv:2305.02301, 2023. 12\\n[230] Y. Gu, L. Dong, F. Wei, and M. Huang, “Minillm: Knowl-\\nedge distillation of large language models,” arXiv preprint\\narXiv:2306.08543, 2023. 12\\n[231] X. L. Li and P. Liang, “Prefix-tuning: Optimizing continu-\\nous prompts for generation,”arXiv preprint arXiv:2101.00190,\\n2021. 13\\n[232] N. Houlsby, A. Giurgiu, S. Jastrzebski, B. Morrone,\\nQ. de Laroussilhe, A. Gesmundo, M. Attariyan, and S. Gelly,\\n“Parameter-efficient transfer learning for nlp,” 2019. 13\\n[233] B. P. Lowerre and B. R. Reddy, “Harpy, a connected speech\\nrecognition system,”The Journal of the Acoustical Society of\\nAmerica, vol. 59, no. S1, pp. S97–S97, 1976. 14\\n[234] A. Graves, “Sequence transduction with recurrent neural net-\\nworks,”arXiv preprint arXiv:1211.3711, 2012. 14\\n[235] H. Sun, M. Haider, R. Zhang, H. Yang, J. Qiu, M. Yin,\\nM. Wang, P. Bartlett, and A. Zanette, “Fast best-of-n decoding\\nvia speculative rejection,” 2024. 14\\n[236] A. Askell, Y. Bai, A. Chen, D. Drain, D. Ganguli, T. Henighan,\\nA. Jones, N. Joseph, B. Mann, N. DasSarma,et al., “A gen-\\neral language assistant as a laboratory for alignment,”arXiv\\npreprint arXiv:2112.00861, 2021. 14\\n[237] A. Glaese, N. McAleese, M. Trębacz, J. Aslanides, V. Firoiu,\\nT. Ewalds, M. Rauh, L. Weidinger, M. Chadwick, P. Thacker,\\net al., “Improving alignment of dialogue agents via targeted\\nhuman judgements,” arXiv preprint arXiv:2209.14375, 2022.\\n14\\n[238] N. Stiennon, L. Ouyang, J. Wu, D. Ziegler, R. Lowe, C. Voss,\\nA. Radford, D. Amodei, and P. F. Christiano, “Learning to\\nsummarize with human feedback,”Advances in Neural Infor-\\nmation Processing Systems, vol. 33, pp. 3008–3021, 2020. 14\\n[239] J. Q. Yang, S. Salamatian, Z. Sun, A. T. Suresh, and\\nA.Beirami,“Asymptoticsoflanguagemodelalignment,” arXiv\\npreprint arXiv:2404.01730, 2024. 14\\n[240] H. Sun, M. Haider, R. Zhang, H. Yang, J. Qiu, M. Yin,\\nM. Wang, P. Bartlett, and A. Zanette, “Fast best-of-n decoding\\nvia speculative rejection,” arXiv preprint arXiv:2410.20290,\\n2024. 14\\n[241] J. Hilton and L. Gao, “Measuring goodhart’s law,”OpenAI\\nResearch Blog, 2022. 14\\n[242] L. Wang, W. Xu, Y. Lan, Z. Hu, Y. Lan, R. K.-W. Lee, and E.-\\nP.Lim,“Plan-and-solveprompting:Improvingzero-shotchain-\\nof-thoughtreasoningbylargelanguagemodels,” arXivpreprint\\narXiv:2305.04091, 2023. 15\\n[243] Y. Wang, Y. Kordi, S. Mishra, A. Liu, N. A. Smith,\\nD. Khashabi, and H. Hajishirzi, “Self-instruct: Aligning lan-\\nguage models with self-generated instructions,”arXiv preprint\\narXiv:2212.10560, 2022. 15\\n[244] X. Chen, R. Aksitov, U. Alon, J. Ren, K. Xiao, P. Yin,\\nS. Prakash, C. Sutton, X. Wang, and D. Zhou, “Universal self-\\nconsistency for large language models,” inICML 2024 Work-\\nshop on In-Context Learning. 15\\n[245] A. Newell, “On the analysis of human problem solving proto-\\ncols,” 1966. 15\\n[246] F. Haji, M. Bethany, M. Tabar, J. Chiang, A. Rios, and\\nP. Najafirad, “Improving llm reasoning with multi-agent tree-\\nof-thought validator agent,”arXiv preprint arXiv:2409.11527,\\n2024. 16\\n[247] M. Besta, N. Blach, A. Kubicek, R. Gerstenberger, M. Pod-\\nstawski, L. Gianinazzi, J. Gajda, T. Lehmann, H. Niewiadom-\\nski, P. Nyczyk, and T. Hoefler, “Graph of thoughts: Solving\\nelaborate problems with large language models,”Proceedings\\nof the AAAI Conference on Artificial Intelligence, vol. 38,\\np. 17682–17690, Mar. 2024. 16\\n[248] D. Wilson, “Llm tree search,” arXiv preprint\\narXiv:2410.19117, 2024. 16\\n[249] D. Hendrycks and K. Gimpel, “A baseline for detecting mis-\\nclassifiedandout-of-distributionexamplesinneuralnetworks,”\\narXiv preprint arXiv:1610.02136, 2016. 16\\n[250] G. Portillo Wightman, A. DeLucia, and M. Dredze, “Strength\\nin numbers: Estimating confidence of large language models\\nby prompt agreement,” inProceedings of the 3rd Workshop on\\nTrustworthy Natural Language Processing (TrustNLP 2023),\\npp. 326–362, 2023. 17\\n[251] J. Qi, H. Tang, and Z. Zhu, “Verifierq: Enhancing llm test\\ntime compute with q-learning-based verifiers,”arXiv preprint\\narXiv:2410.08048, 2024. 17\\n[252] A. Madaan, N. Tandon, P. Gupta, S. Hallinan, L. Gao,\\nS. Wiegreffe, U. Alon, N. Dziri, S. Prabhumoye, Y. Yang,\\nS. Gupta, B. P. Majumder, K. Hermann, S. Welleck, A. Yaz-\\ndanbakhsh,andP.Clark,“Self-refine:Iterativerefinementwith\\nself-feedback,” 2023. 17\\n[253] Z. Xi, W. Chen, X. Guo, W. He, Y. Ding, B. Hong, M. Zhang,\\nJ. Wang, S. Jin, E. Zhou,et al., “The rise and potential of\\nlarge language model based agents: A survey,”arXiv preprint\\narXiv:2309.07864, 2023. 17\\n[254] R. Coulom, “Efficient selectivity and backup operators in\\nmonte-carlo tree search,” inInternational conference on com-\\nputers and games, pp. 72–83, Springer, 2006. 17\\n[255] J. X. Chen, “The evolution of computing: Alphago,”Comput-\\ning in Science & Engineering, vol. 18, no. 4, pp. 4–7, 2016. 17\\n[256] L. Kocsis and C. Szepesvári, “Bandit based monte-carlo plan-\\nning,” inEuropean conference on machine learning, pp. 282–\\n293, Springer, 2006. 17\\n[257] H. W. Sprueill, C. Edwards, M. V. Olarte, U. Sanyal, H. Ji, and\\nS. Choudhury, “Monte carlo thought search: Large language\\nmodel querying for complex scientific reasoning in catalyst\\ndesign,” arXiv preprint arXiv:2310.14420, 2023. 18, 20\\n[258] M. DeLorenzo, A. B. Chowdhury, V. Gohil, S. Thakur,\\nR. Karri, S. Garg, and J. Rajendran, “Make every move count:\\nLlm-based high-quality rtl code generation using mcts,”arXiv\\npreprint arXiv:2402.03289, 2024. 18\\n[259] S. Park, X. Liu, Y. Gong, and E. Choi, “Ensembling large\\nlanguage models with process reward-guided tree search for\\nbetter complex reasoning,”arXiv preprint arXiv:2412.15797,\\n2024. 18\\n[260] M. Shen, G. Zeng, Z. Qi, Z.-W. Hong, Z. Chen, W. Lu,\\nG.Wornell,S.Das,D.Cox,andC.Gan,“Satori:Reinforcement\\nlearning with chain-of-action-thought enhances llm reasoning\\nvia autoregressive search,”arXiv preprint arXiv:2502.02508,\\n2025. 18\\n[261] S. R. Motwani, C. Smith, R. J. Das, R. Rafailov, I. Laptev,\\nP. H. S. Torr, F. Pizzati, R. Clark, and C. S. de Witt, “Malt:\\nImproving reasoning with multi-agent llm training,” 2025. 18\\n[262] U.Anwar,A.Saparov,J.Rando,D.Paleka,M.Turpin,P.Hase,\\nE. S. Lubana, E. Jenner, S. Casper, O. Sourbut,et al., “Foun-\\ndational challenges in assuring alignment and safety of large\\nlanguage models,”arXiv preprint arXiv:2404.09932, 2024. 18\\n[263] W. Zhang, P. H. Torr, M. Elhoseiny, and A. Bibi, “Bi-factorial\\npreference optimization: Balancing safety-helpfulness in lan-\\nguage models,”arXiv preprint arXiv:2408.15313, 2024. 18\\n[264] C. Li, W. Wu, H. Zhang, Y. Xia, S. Mao, L. Dong, I. Vulić,\\nand F. Wei, “Imagine while reasoning in space: Multimodal\\nvisualization-of-thought,” arXiv preprint arXiv:2501.07542,\\n2025. 18\\n[265] F. Nowak, A. Svete, A. Butoi, and R. Cotterell, “On the\\nrepresentationalcapacityofneurallanguagemodelswithchain-\\nof-thought reasoning,”arXiv preprint arXiv:2406.14197, 2024.\\n18']}\n",
      "\n",
      "---\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">╭───────────────────────────────────────────────────── INFO ──────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Total number of conversation turns up to this point: 0</span>                                                          <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33m╭─\u001b[0m\u001b[33m────────────────────────────────────────────────────\u001b[0m\u001b[33m INFO \u001b[0m\u001b[33m─────────────────────────────────────────────────────\u001b[0m\u001b[33m─╮\u001b[0m\n",
       "\u001b[33m│\u001b[0m \u001b[37mTotal number of conversation turns up to this point: 0\u001b[0m                                                          \u001b[33m│\u001b[0m\n",
       "\u001b[33m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">╭───────────────────────────────────────────────────── INFO ──────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Calling prompt_initialize</span>                                                                                       <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33m╭─\u001b[0m\u001b[33m────────────────────────────────────────────────────\u001b[0m\u001b[33m INFO \u001b[0m\u001b[33m─────────────────────────────────────────────────────\u001b[0m\u001b[33m─╮\u001b[0m\n",
       "\u001b[33m│\u001b[0m \u001b[37mCalling prompt_initialize\u001b[0m                                                                                       \u001b[33m│\u001b[0m\n",
       "\u001b[33m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">╭───────────────────────────────────────────────────── INFO ──────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">System Prompt: </span>                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">You are an advanced synthetic data generator, engineered to produce high-quality, task-specific syn</span>             <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">User Prompt: You are tasked to help me generate a dataset of 2 rows entirely in Vietnamese, based entirely on </span>  <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">the</span>                                                                                                             <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33m╭─\u001b[0m\u001b[33m────────────────────────────────────────────────────\u001b[0m\u001b[33m INFO \u001b[0m\u001b[33m─────────────────────────────────────────────────────\u001b[0m\u001b[33m─╮\u001b[0m\n",
       "\u001b[33m│\u001b[0m \u001b[37mSystem Prompt: \u001b[0m                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m \u001b[37mYou are an advanced synthetic data generator, engineered to produce high-quality, task-specific syn\u001b[0m             \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m \u001b[37mUser Prompt: You are tasked to help me generate a dataset of 2 rows entirely in Vietnamese, based entirely on \u001b[0m  \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m \u001b[37mthe\u001b[0m                                                                                                             \u001b[33m│\u001b[0m\n",
       "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
       "\u001b[33m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">╭──────────────────────────────────────────────── OUTPUT_MESSAGE ─────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span>                                                                                                                 <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">---------FISH_FOR_FEEDBACK---------</span>                                                                             <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span>                                                                                                                 <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">System Prompt:</span>                                                                                                  <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">You are an advanced synthetic data generator, engineered to produce high-quality, task-specific synthetic </span>      <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">datasets. Your mission is to generate data samples in formats that precisely adhere to the requirements </span>        <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">provided.</span>                                                                                                       <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span>                                                                                                                 <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">User Prompt:You are tasked to help me generate a dataset of 2 rows entirely in Vietnamese, based entirely on </span>   <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">the following context:</span>                                                                                          <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">- 13</span>                                                                                                            <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Model Category Source Description</span>                                                                               <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">1.Parameter-EfficientFine-Tuning&amp;ModelCompression</span>                                                               <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">LoRA[60] Low-Rank Adaptation Link Injects trainable low-rank adapters for efficient fine-tuning.</span>                <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">QLoRA[188] Quantized Adaptation Link Combines 4-bit quantization with LoRA to enable fine-tuning on consumer </span>   <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">GPUs</span>                                                                                                            <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">GPTQ[189] Post-Training Quantization Link Optimal 4-bit quantization method for GPT-style models with minimal </span>  <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">loss</span>                                                                                                            <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">SparseGPT[190] Pruning Link One-shot pruning that preserves model quality with compensation.</span>                    <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">PEFT(HF) [191] Unified Fine-Tuning Link Library integrating LoRA, prefix tuning, and other parameter-efficient </span> <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">methods</span>                                                                                                         <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">BitsAndBytes[192] Low-Precision Training Link Enables 8-bit optimizers and 4-bit quantization for </span>              <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">memory-efficient training</span>                                                                                       <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">AdaLoRA[193] Adaptive Adaptation Link Dynamically allocates parameter budget between layers during fine-tuning</span>  <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">P-Tuningv2 [194] Prompt Optimization Link Learns continuous prompt embeddings through deep prompt tuning</span>        <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">2.DataManagement&amp;Preprocessing</span>                                                                                  <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">HFDatasets [195] Data Processing Link Unified API for 30k+ datasets with streaming, versioning, and </span>            <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">preprocessing</span>                                                                                                   <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">WebDataset[196] Data Streaming Link Efficient tar-based sharding format for petascale distributed training</span>      <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">DVC[197] Data Versioning Link Git-like version control for datasets and machine learning pipelines</span>              <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">ApacheArrow [198] Memory Format Link Language-agnostic columnar memory format for zero-copy data access</span>         <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Zstandard[199] Compression Link High-speed compression algorithm for training data storage/transfer</span>             <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Cleanlab[200] Data Quality Link Automatic detection of label errors and outliers in training datasets</span>           <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">3.DistributedTraining&amp;Optimization</span>                                                                              <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">DeepSpeed[201] Training Optimization Link ZeRO parallelism, 3D parallelism, and memory optimizations for giant </span> <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">models</span>                                                                                                          <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Megatron-LM[202] Model Parallelism Link NVIDIA’s optimized framework for large transformer model training</span>       <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Colossal-AI[203] Heterogeneous Training Link Unified system supporting multiple parallelization strategies</span>      <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Horovod[204] Distributed Training Link MPI-inspired framework for multi-GPU/multi-node synchronization</span>          <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Ray[205] Distributed Computing Link Universal framework for distributed Python applications at scale</span>            <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">4.EfficientInference&amp;Deployment</span>                                                                                 <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">vLLM[206] Serving Optimization Link Paged attention implementation for high-throughput LLM serving</span>              <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">TensorRT[207] GPU Optimization Link NVIDIA’s inference optimizer with kernel fusion and quantization support</span>    <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Triton[208] Serving Framework Link Production-grade serving with concurrent model execution support</span>             <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">ONNX[209] Cross-Platform Link Unified inference engine with hardware-specific optimizations</span>                     <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">OpenVINO[210] Intel Optimization Link Runtime for Intel CPUs/iGPUs with pruning/quantization support</span>            <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">XNNPACK[211] Mobile Inference Link Highly optimized floating-point kernels for ARM CPUs</span>                         <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Groq [212] AI Accelerator Link Deterministic low-latency inference via custom tensor streaming processor</span>        <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">5.IntegratedDevelopmentEcosystems</span>                                                                               <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">HFEcosystem [213] Full Stack Link Transformers + Datasets + Accelerate + Inference Endpoints</span>                    <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">DeepSpeed[201] Training/Inference Link Microsoft’s end-to-end solution for billion-parameter models</span>             <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">PyTorch[214] Unified Framework Link Native LLM support via torch.compile and scaled dot-product attention</span>       <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">LLMReasoners [215] Advanced Reasoning Link Enhances LLM reasoning capabilities using advanced search </span>           <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">algorithms.</span>                                                                                                     <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">TABLE 2: Comprehensive Overview of Modern LLM Methods and Frameworks.</span>                                           <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">4.6 Preference and Alignment SFT</span>                                                                                <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">While RLHF is not purely supervised, it starts with a su-</span>                                                       <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">pervised preferenceor alignment finetuning stage. This stage</span>                                                    <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">uses human-labeled or human-ranked examples to teach the</span>                                                        <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">model about desirable vs. undesirable outputs (e.g., safe vs.</span>                                                   <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">toxic). By training on these explicit preferences, the model</span>                                                    <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">becomes more aligned with user values, reducing harmful or</span>                                                      <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">off-topic completions. Works like InstructGPT [58] illustrate</span>                                                   <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">howsupervisedpreferencedataiscriticalbeforerewardmodel</span>                                                          <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">training andRL updates begin.</span>                                                                                   <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">4.7 Efficient Finetuning</span>                                                                                        <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Fully finetuning aLLM can be computationally and memory-</span>                                                        <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">intensive, particularly as model sizes grow into the tens or</span>                                                    <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">hundreds of billions of parameters. To address these chal-</span>                                                      <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">lenges, parameter-efficient finetuning (PEFT) techniques intro-</span>                                                 <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">duce a small set of trainable parameters or learnable prompts</span>                                                   <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">while leaving most of the model weights frozen. Approaches</span>                                                      <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">such as LoRA [60], Prefix Tuning [231], and Adapters [232]</span>                                                      <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">exemplify this strategy by injecting lightweight modules (or</span>                                                    <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">prompts) in specific layers, thus significantly reducing the</span>                                                    <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">memory footprint.</span>                                                                                               <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Figure 4 illustrates how these techniques fit into a broader</span>                                                    <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">ecosystemthatinvolvessystem-leveloptimizations,dataman-</span>                                                         <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">agement, and evaluation strategies for LLMs. In particular,</span>                                                     <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">PEFT approaches can be combined with quantization and</span>                                                           <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">pruning methods [190, 188] to further minimize memory</span>                                                           <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">usage and compute overhead, enabling finetuning on smaller</span>                                                      <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">GPUs or even consumer-grade hardware. For instance,QLoRA</span>                                                        <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">unifies 4-bit quantization with low-rank adaptation, while</span>                                                      <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">BitsAndBytes provides 8-bit optimizers to makeLLM training</span>                                                      <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">more practical in constrained environments (Table 2).</span>                                                           <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Moreover, these PEFT methods still require supervised</span>                                                           <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">data to guide the adaptation process, but the reduction in</span>                                                      <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">the number of trainable parameters makes it more feasible</span>                                                       <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">to use in-domain or task-specific datasets. This is especially</span>                                                  <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">valuable for specialized domains (e.g., medical or software</span>                                                     <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">development), where data might be limited or expensive to</span>                                                       <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">annotate. As shown in Table 2,PEFT (HF) integrates several</span>                                                      <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">of these approaches (LoRA, prefix tuning, and more) into a</span>                                                      <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">single library, streamlining deployment in both research and</span>                                                    <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">production settings.</span>                                                                                            <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Combining efficient tuning designs likeLoRA</span>                                                                     <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">and QLoRA with system and data optimizations</span>                                                                    <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">(Figure4)enablescost-effectiveLLMadaptation</span>                                                                     <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">for tasks like domain-specific text generation,</span>                                                                 <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">without expensive full fine-tuning.</span>                                                                             <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">5 Test-time Scaling Methods</span>                                                                                     <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">WhileRL fine-tunes the model’s policy, test-time scaling (TTS)</span>                                                  <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">enhances reasoning during inference typically without model</span>                                                     <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span>                                                                                                                 <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">You must strictly follow the below format for this task:</span>                                                        <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">[</span>                                                                                                               <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">  {</span>                                                                                                             <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">    \"prompt\": \"Your generated prompt\",</span>                                                                          <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">    \"chosen\": \"Chosen completion text\",</span>                                                                         <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">    \"rejected\": \"Rejected completion text\"</span>                                                                      <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">  },</span>                                                                                                            <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">  ...</span>                                                                                                           <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">]</span>                                                                                                               <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span>                                                                                                                 <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Notes:</span>                                                                                                          <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">- Both \"prompt\", \"chosen\" and \"rejected\" fields must be non-empty. \"Chosen\" answer must be in high quality and </span> <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">long enough.</span>                                                                                                    <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">- Each sample must be a JSON dictionary with two keys: \"prompt\" and \"completion\".</span>                               <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">- You MUST ONLY return the output text with the above format and nothing else.</span>                                  <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span>                                                                                                                 <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Additional Dataset Info: I want to use your outputs to train a AI Researcher Model</span>                              <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">.</span>                                                                                                               <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m╭─\u001b[0m\u001b[32m───────────────────────────────────────────────\u001b[0m\u001b[32m OUTPUT_MESSAGE \u001b[0m\u001b[32m────────────────────────────────────────────────\u001b[0m\u001b[32m─╮\u001b[0m\n",
       "\u001b[32m│\u001b[0m                                                                                                                 \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37m---------FISH_FOR_FEEDBACK---------\u001b[0m                                                                             \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m                                                                                                                 \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37mSystem Prompt:\u001b[0m                                                                                                  \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37mYou are an advanced synthetic data generator, engineered to produce high-quality, task-specific synthetic \u001b[0m      \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37mdatasets. Your mission is to generate data samples in formats that precisely adhere to the requirements \u001b[0m        \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37mprovided.\u001b[0m                                                                                                       \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m                                                                                                                 \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37mUser Prompt:You are tasked to help me generate a dataset of 2 rows entirely in Vietnamese, based entirely on \u001b[0m   \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37mthe following context:\u001b[0m                                                                                          \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37m- 13\u001b[0m                                                                                                            \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37mModel Category Source Description\u001b[0m                                                                               \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37m1.Parameter-EfficientFine-Tuning&ModelCompression\u001b[0m                                                               \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37mLoRA[60] Low-Rank Adaptation Link Injects trainable low-rank adapters for efficient fine-tuning.\u001b[0m                \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37mQLoRA[188] Quantized Adaptation Link Combines 4-bit quantization with LoRA to enable fine-tuning on consumer \u001b[0m   \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37mGPUs\u001b[0m                                                                                                            \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37mGPTQ[189] Post-Training Quantization Link Optimal 4-bit quantization method for GPT-style models with minimal \u001b[0m  \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37mloss\u001b[0m                                                                                                            \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37mSparseGPT[190] Pruning Link One-shot pruning that preserves model quality with compensation.\u001b[0m                    \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37mPEFT(HF) [191] Unified Fine-Tuning Link Library integrating LoRA, prefix tuning, and other parameter-efficient \u001b[0m \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37mmethods\u001b[0m                                                                                                         \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37mBitsAndBytes[192] Low-Precision Training Link Enables 8-bit optimizers and 4-bit quantization for \u001b[0m              \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37mmemory-efficient training\u001b[0m                                                                                       \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37mAdaLoRA[193] Adaptive Adaptation Link Dynamically allocates parameter budget between layers during fine-tuning\u001b[0m  \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37mP-Tuningv2 [194] Prompt Optimization Link Learns continuous prompt embeddings through deep prompt tuning\u001b[0m        \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37m2.DataManagement&Preprocessing\u001b[0m                                                                                  \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37mHFDatasets [195] Data Processing Link Unified API for 30k+ datasets with streaming, versioning, and \u001b[0m            \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37mpreprocessing\u001b[0m                                                                                                   \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37mWebDataset[196] Data Streaming Link Efficient tar-based sharding format for petascale distributed training\u001b[0m      \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37mDVC[197] Data Versioning Link Git-like version control for datasets and machine learning pipelines\u001b[0m              \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37mApacheArrow [198] Memory Format Link Language-agnostic columnar memory format for zero-copy data access\u001b[0m         \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37mZstandard[199] Compression Link High-speed compression algorithm for training data storage/transfer\u001b[0m             \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37mCleanlab[200] Data Quality Link Automatic detection of label errors and outliers in training datasets\u001b[0m           \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37m3.DistributedTraining&Optimization\u001b[0m                                                                              \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37mDeepSpeed[201] Training Optimization Link ZeRO parallelism, 3D parallelism, and memory optimizations for giant \u001b[0m \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37mmodels\u001b[0m                                                                                                          \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37mMegatron-LM[202] Model Parallelism Link NVIDIA’s optimized framework for large transformer model training\u001b[0m       \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37mColossal-AI[203] Heterogeneous Training Link Unified system supporting multiple parallelization strategies\u001b[0m      \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37mHorovod[204] Distributed Training Link MPI-inspired framework for multi-GPU/multi-node synchronization\u001b[0m          \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37mRay[205] Distributed Computing Link Universal framework for distributed Python applications at scale\u001b[0m            \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37m4.EfficientInference&Deployment\u001b[0m                                                                                 \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37mvLLM[206] Serving Optimization Link Paged attention implementation for high-throughput LLM serving\u001b[0m              \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37mTensorRT[207] GPU Optimization Link NVIDIA’s inference optimizer with kernel fusion and quantization support\u001b[0m    \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37mTriton[208] Serving Framework Link Production-grade serving with concurrent model execution support\u001b[0m             \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37mONNX[209] Cross-Platform Link Unified inference engine with hardware-specific optimizations\u001b[0m                     \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37mOpenVINO[210] Intel Optimization Link Runtime for Intel CPUs/iGPUs with pruning/quantization support\u001b[0m            \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37mXNNPACK[211] Mobile Inference Link Highly optimized floating-point kernels for ARM CPUs\u001b[0m                         \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37mGroq [212] AI Accelerator Link Deterministic low-latency inference via custom tensor streaming processor\u001b[0m        \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37m5.IntegratedDevelopmentEcosystems\u001b[0m                                                                               \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37mHFEcosystem [213] Full Stack Link Transformers + Datasets + Accelerate + Inference Endpoints\u001b[0m                    \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37mDeepSpeed[201] Training/Inference Link Microsoft’s end-to-end solution for billion-parameter models\u001b[0m             \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37mPyTorch[214] Unified Framework Link Native LLM support via torch.compile and scaled dot-product attention\u001b[0m       \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37mLLMReasoners [215] Advanced Reasoning Link Enhances LLM reasoning capabilities using advanced search \u001b[0m           \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37malgorithms.\u001b[0m                                                                                                     \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37mTABLE 2: Comprehensive Overview of Modern LLM Methods and Frameworks.\u001b[0m                                           \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37m4.6 Preference and Alignment SFT\u001b[0m                                                                                \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37mWhile RLHF is not purely supervised, it starts with a su-\u001b[0m                                                       \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37mpervised preferenceor alignment finetuning stage. This stage\u001b[0m                                                    \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37muses human-labeled or human-ranked examples to teach the\u001b[0m                                                        \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37mmodel about desirable vs. undesirable outputs (e.g., safe vs.\u001b[0m                                                   \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37mtoxic). By training on these explicit preferences, the model\u001b[0m                                                    \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37mbecomes more aligned with user values, reducing harmful or\u001b[0m                                                      \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37moff-topic completions. Works like InstructGPT [58] illustrate\u001b[0m                                                   \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37mhowsupervisedpreferencedataiscriticalbeforerewardmodel\u001b[0m                                                          \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37mtraining andRL updates begin.\u001b[0m                                                                                   \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37m4.7 Efficient Finetuning\u001b[0m                                                                                        \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37mFully finetuning aLLM can be computationally and memory-\u001b[0m                                                        \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37mintensive, particularly as model sizes grow into the tens or\u001b[0m                                                    \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37mhundreds of billions of parameters. To address these chal-\u001b[0m                                                      \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37mlenges, parameter-efficient finetuning (PEFT) techniques intro-\u001b[0m                                                 \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37mduce a small set of trainable parameters or learnable prompts\u001b[0m                                                   \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37mwhile leaving most of the model weights frozen. Approaches\u001b[0m                                                      \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37msuch as LoRA [60], Prefix Tuning [231], and Adapters [232]\u001b[0m                                                      \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37mexemplify this strategy by injecting lightweight modules (or\u001b[0m                                                    \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37mprompts) in specific layers, thus significantly reducing the\u001b[0m                                                    \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37mmemory footprint.\u001b[0m                                                                                               \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37mFigure 4 illustrates how these techniques fit into a broader\u001b[0m                                                    \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37mecosystemthatinvolvessystem-leveloptimizations,dataman-\u001b[0m                                                         \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37magement, and evaluation strategies for LLMs. In particular,\u001b[0m                                                     \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37mPEFT approaches can be combined with quantization and\u001b[0m                                                           \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37mpruning methods [190, 188] to further minimize memory\u001b[0m                                                           \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37musage and compute overhead, enabling finetuning on smaller\u001b[0m                                                      \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37mGPUs or even consumer-grade hardware. For instance,QLoRA\u001b[0m                                                        \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37munifies 4-bit quantization with low-rank adaptation, while\u001b[0m                                                      \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37mBitsAndBytes provides 8-bit optimizers to makeLLM training\u001b[0m                                                      \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37mmore practical in constrained environments (Table 2).\u001b[0m                                                           \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37mMoreover, these PEFT methods still require supervised\u001b[0m                                                           \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37mdata to guide the adaptation process, but the reduction in\u001b[0m                                                      \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37mthe number of trainable parameters makes it more feasible\u001b[0m                                                       \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37mto use in-domain or task-specific datasets. This is especially\u001b[0m                                                  \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37mvaluable for specialized domains (e.g., medical or software\u001b[0m                                                     \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37mdevelopment), where data might be limited or expensive to\u001b[0m                                                       \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37mannotate. As shown in Table 2,PEFT (HF) integrates several\u001b[0m                                                      \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37mof these approaches (LoRA, prefix tuning, and more) into a\u001b[0m                                                      \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37msingle library, streamlining deployment in both research and\u001b[0m                                                    \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37mproduction settings.\u001b[0m                                                                                            \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37mCombining efficient tuning designs likeLoRA\u001b[0m                                                                     \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37mand QLoRA with system and data optimizations\u001b[0m                                                                    \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37m(Figure4)enablescost-effectiveLLMadaptation\u001b[0m                                                                     \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37mfor tasks like domain-specific text generation,\u001b[0m                                                                 \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37mwithout expensive full fine-tuning.\u001b[0m                                                                             \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37m5 Test-time Scaling Methods\u001b[0m                                                                                     \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37mWhileRL fine-tunes the model’s policy, test-time scaling (TTS)\u001b[0m                                                  \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37menhances reasoning during inference typically without model\u001b[0m                                                     \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m                                                                                                                 \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37mYou must strictly follow the below format for this task:\u001b[0m                                                        \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37m[\u001b[0m                                                                                                               \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37m  {\u001b[0m                                                                                                             \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37m    \"prompt\": \"Your generated prompt\",\u001b[0m                                                                          \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37m    \"chosen\": \"Chosen completion text\",\u001b[0m                                                                         \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37m    \"rejected\": \"Rejected completion text\"\u001b[0m                                                                      \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37m  },\u001b[0m                                                                                                            \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37m  ...\u001b[0m                                                                                                           \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37m]\u001b[0m                                                                                                               \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m                                                                                                                 \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37mNotes:\u001b[0m                                                                                                          \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37m- Both \"prompt\", \"chosen\" and \"rejected\" fields must be non-empty. \"Chosen\" answer must be in high quality and \u001b[0m \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37mlong enough.\u001b[0m                                                                                                    \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37m- Each sample must be a JSON dictionary with two keys: \"prompt\" and \"completion\".\u001b[0m                               \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37m- You MUST ONLY return the output text with the above format and nothing else.\u001b[0m                                  \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m                                                                                                                 \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37mAdditional Dataset Info: I want to use your outputs to train a AI Researcher Model\u001b[0m                              \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37m.\u001b[0m                                                                                                               \u001b[32m│\u001b[0m\n",
       "\u001b[32m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">╭──────────────────────────────────────────────── OUTPUT_MESSAGE ─────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span>                                                                                                                 <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">---------FISH_FOR_FEEDBACK---------</span>                                                                             <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span>                                                                                                                 <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Response:```json</span>                                                                                                <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">[</span>                                                                                                               <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">  {</span>                                                                                                             <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">    \"prompt\": \"Hãy giải thích ngắn gọn về LoRA và QLoRA trong lĩnh vực tinh chỉnh mô hình ngôn ngữ lớn (LLM).\",</span> <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">    \"chosen\": \"LoRA (Low-Rank Adaptation) là một phương pháp tinh chỉnh hiệu quả tham số, bằng cách chèn các </span>   <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">adapter có thứ hạng thấp có thể huấn luyện được vào mô hình. QLoRA (Quantized Adaptation) kết hợp lượng tử hóa </span> <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">4-bit với LoRA, cho phép tinh chỉnh trên các GPU tiêu dùng với chi phí bộ nhớ thấp hơn đáng kể. Cả hai đều giúp</span> <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">giảm đáng kể lượng tài nguyên tính toán cần thiết để điều chỉnh LLM cho các tác vụ cụ thể.\",</span>                    <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">    \"rejected\": \"LoRA và QLoRA là các thuật toán nén dữ liệu.\"</span>                                                  <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">  },</span>                                                                                                            <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">  {</span>                                                                                                             <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">    \"prompt\": \"Mô tả vai trò của HF Datasets và WebDataset trong quản lý và tiền xử lý dữ liệu cho huấn luyện </span>  <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">mô hình ngôn ngữ lớn.\",</span>                                                                                         <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">    \"chosen\": \"HF Datasets cung cấp một API thống nhất cho hơn 30.000 bộ dữ liệu, hỗ trợ streaming, quản lý </span>    <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">phiên bản và tiền xử lý dữ liệu. WebDataset là một định dạng phân mảnh dựa trên tar hiệu quả, được thiết kế cho</span> <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">huấn luyện phân tán quy mô petabyte. HF Datasets giúp truy cập và quản lý dữ liệu dễ dàng hơn, trong khi </span>       <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">WebDataset tối ưu hóa hiệu suất đọc/ghi dữ liệu trong quá trình huấn luyện phân tán.\",</span>                          <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">    \"rejected\": \"HF Datasets và WebDataset là các công cụ để tạo ra dữ liệu giả.\"</span>                               <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">  }</span>                                                                                                             <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">]</span>                                                                                                               <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">│</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">```</span>                                                                                                             <span style=\"color: #008000; text-decoration-color: #008000\">│</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m╭─\u001b[0m\u001b[32m───────────────────────────────────────────────\u001b[0m\u001b[32m OUTPUT_MESSAGE \u001b[0m\u001b[32m────────────────────────────────────────────────\u001b[0m\u001b[32m─╮\u001b[0m\n",
       "\u001b[32m│\u001b[0m                                                                                                                 \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37m---------FISH_FOR_FEEDBACK---------\u001b[0m                                                                             \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m                                                                                                                 \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37mResponse:```json\u001b[0m                                                                                                \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37m[\u001b[0m                                                                                                               \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37m  {\u001b[0m                                                                                                             \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37m    \"prompt\": \"Hãy giải thích ngắn gọn về LoRA và QLoRA trong lĩnh vực tinh chỉnh mô hình ngôn ngữ lớn (LLM).\",\u001b[0m \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37m    \"chosen\": \"LoRA (Low-Rank Adaptation) là một phương pháp tinh chỉnh hiệu quả tham số, bằng cách chèn các \u001b[0m   \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37madapter có thứ hạng thấp có thể huấn luyện được vào mô hình. QLoRA (Quantized Adaptation) kết hợp lượng tử hóa \u001b[0m \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37m4-bit với LoRA, cho phép tinh chỉnh trên các GPU tiêu dùng với chi phí bộ nhớ thấp hơn đáng kể. Cả hai đều giúp\u001b[0m \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37mgiảm đáng kể lượng tài nguyên tính toán cần thiết để điều chỉnh LLM cho các tác vụ cụ thể.\",\u001b[0m                    \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37m    \"rejected\": \"LoRA và QLoRA là các thuật toán nén dữ liệu.\"\u001b[0m                                                  \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37m  },\u001b[0m                                                                                                            \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37m  {\u001b[0m                                                                                                             \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37m    \"prompt\": \"Mô tả vai trò của HF Datasets và WebDataset trong quản lý và tiền xử lý dữ liệu cho huấn luyện \u001b[0m  \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37mmô hình ngôn ngữ lớn.\",\u001b[0m                                                                                         \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37m    \"chosen\": \"HF Datasets cung cấp một API thống nhất cho hơn 30.000 bộ dữ liệu, hỗ trợ streaming, quản lý \u001b[0m    \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37mphiên bản và tiền xử lý dữ liệu. WebDataset là một định dạng phân mảnh dựa trên tar hiệu quả, được thiết kế cho\u001b[0m \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37mhuấn luyện phân tán quy mô petabyte. HF Datasets giúp truy cập và quản lý dữ liệu dễ dàng hơn, trong khi \u001b[0m       \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37mWebDataset tối ưu hóa hiệu suất đọc/ghi dữ liệu trong quá trình huấn luyện phân tán.\",\u001b[0m                          \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37m    \"rejected\": \"HF Datasets và WebDataset là các công cụ để tạo ra dữ liệu giả.\"\u001b[0m                               \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37m  }\u001b[0m                                                                                                             \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37m]\u001b[0m                                                                                                               \u001b[32m│\u001b[0m\n",
       "\u001b[32m│\u001b[0m \u001b[37m```\u001b[0m                                                                                                             \u001b[32m│\u001b[0m\n",
       "\u001b[32m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': '```json\\n[\\n  {\\n    \"prompt\": \"Hãy giải thích ngắn gọn về LoRA và QLoRA trong lĩnh vực tinh chỉnh mô hình ngôn ngữ lớn (LLM).\",\\n    \"chosen\": \"LoRA (Low-Rank Adaptation) là một phương pháp tinh chỉnh hiệu quả tham số, bằng cách chèn các adapter có thứ hạng thấp có thể huấn luyện được vào mô hình. QLoRA (Quantized Adaptation) kết hợp lượng tử hóa 4-bit với LoRA, cho phép tinh chỉnh trên các GPU tiêu dùng với chi phí bộ nhớ thấp hơn đáng kể. Cả hai đều giúp giảm đáng kể lượng tài nguyên tính toán cần thiết để điều chỉnh LLM cho các tác vụ cụ thể.\",\\n    \"rejected\": \"LoRA và QLoRA là các thuật toán nén dữ liệu.\"\\n  },\\n  {\\n    \"prompt\": \"Mô tả vai trò của HF Datasets và WebDataset trong quản lý và tiền xử lý dữ liệu cho huấn luyện mô hình ngôn ngữ lớn.\",\\n    \"chosen\": \"HF Datasets cung cấp một API thống nhất cho hơn 30.000 bộ dữ liệu, hỗ trợ streaming, quản lý phiên bản và tiền xử lý dữ liệu. WebDataset là một định dạng phân mảnh dựa trên tar hiệu quả, được thiết kế cho huấn luyện phân tán quy mô petabyte. HF Datasets giúp truy cập và quản lý dữ liệu dễ dàng hơn, trong khi WebDataset tối ưu hóa hiệu suất đọc/ghi dữ liệu trong quá trình huấn luyện phân tán.\",\\n    \"rejected\": \"HF Datasets và WebDataset là các công cụ để tạo ra dữ liệu giả.\"\\n  }\\n]\\n```', 'conversations': [{'role': 'system', 'content': '\\nYou are an advanced synthetic data generator, engineered to produce high-quality, task-specific synthetic datasets. Your mission is to generate data samples in formats that precisely adhere to the requirements provided.\\n'}, {'role': 'user', 'content': 'You are tasked to help me generate a dataset of 2 rows entirely in Vietnamese, based entirely on the following context:\\n- 13\\nModel Category Source Description\\n1.Parameter-EfficientFine-Tuning&ModelCompression\\nLoRA[60] Low-Rank Adaptation Link Injects trainable low-rank adapters for efficient fine-tuning.\\nQLoRA[188] Quantized Adaptation Link Combines 4-bit quantization with LoRA to enable fine-tuning on consumer GPUs\\nGPTQ[189] Post-Training Quantization Link Optimal 4-bit quantization method for GPT-style models with minimal loss\\nSparseGPT[190] Pruning Link One-shot pruning that preserves model quality with compensation.\\nPEFT(HF) [191] Unified Fine-Tuning Link Library integrating LoRA, prefix tuning, and other parameter-efficient methods\\nBitsAndBytes[192] Low-Precision Training Link Enables 8-bit optimizers and 4-bit quantization for memory-efficient training\\nAdaLoRA[193] Adaptive Adaptation Link Dynamically allocates parameter budget between layers during fine-tuning\\nP-Tuningv2 [194] Prompt Optimization Link Learns continuous prompt embeddings through deep prompt tuning\\n2.DataManagement&Preprocessing\\nHFDatasets [195] Data Processing Link Unified API for 30k+ datasets with streaming, versioning, and preprocessing\\nWebDataset[196] Data Streaming Link Efficient tar-based sharding format for petascale distributed training\\nDVC[197] Data Versioning Link Git-like version control for datasets and machine learning pipelines\\nApacheArrow [198] Memory Format Link Language-agnostic columnar memory format for zero-copy data access\\nZstandard[199] Compression Link High-speed compression algorithm for training data storage/transfer\\nCleanlab[200] Data Quality Link Automatic detection of label errors and outliers in training datasets\\n3.DistributedTraining&Optimization\\nDeepSpeed[201] Training Optimization Link ZeRO parallelism, 3D parallelism, and memory optimizations for giant models\\nMegatron-LM[202] Model Parallelism Link NVIDIA’s optimized framework for large transformer model training\\nColossal-AI[203] Heterogeneous Training Link Unified system supporting multiple parallelization strategies\\nHorovod[204] Distributed Training Link MPI-inspired framework for multi-GPU/multi-node synchronization\\nRay[205] Distributed Computing Link Universal framework for distributed Python applications at scale\\n4.EfficientInference&Deployment\\nvLLM[206] Serving Optimization Link Paged attention implementation for high-throughput LLM serving\\nTensorRT[207] GPU Optimization Link NVIDIA’s inference optimizer with kernel fusion and quantization support\\nTriton[208] Serving Framework Link Production-grade serving with concurrent model execution support\\nONNX[209] Cross-Platform Link Unified inference engine with hardware-specific optimizations\\nOpenVINO[210] Intel Optimization Link Runtime for Intel CPUs/iGPUs with pruning/quantization support\\nXNNPACK[211] Mobile Inference Link Highly optimized floating-point kernels for ARM CPUs\\nGroq [212] AI Accelerator Link Deterministic low-latency inference via custom tensor streaming processor\\n5.IntegratedDevelopmentEcosystems\\nHFEcosystem [213] Full Stack Link Transformers + Datasets + Accelerate + Inference Endpoints\\nDeepSpeed[201] Training/Inference Link Microsoft’s end-to-end solution for billion-parameter models\\nPyTorch[214] Unified Framework Link Native LLM support via torch.compile and scaled dot-product attention\\nLLMReasoners [215] Advanced Reasoning Link Enhances LLM reasoning capabilities using advanced search algorithms.\\nTABLE 2: Comprehensive Overview of Modern LLM Methods and Frameworks.\\n4.6 Preference and Alignment SFT\\nWhile RLHF is not purely supervised, it starts with a su-\\npervised preferenceor alignment finetuning stage. This stage\\nuses human-labeled or human-ranked examples to teach the\\nmodel about desirable vs. undesirable outputs (e.g., safe vs.\\ntoxic). By training on these explicit preferences, the model\\nbecomes more aligned with user values, reducing harmful or\\noff-topic completions. Works like InstructGPT [58] illustrate\\nhowsupervisedpreferencedataiscriticalbeforerewardmodel\\ntraining andRL updates begin.\\n4.7 Efficient Finetuning\\nFully finetuning aLLM can be computationally and memory-\\nintensive, particularly as model sizes grow into the tens or\\nhundreds of billions of parameters. To address these chal-\\nlenges, parameter-efficient finetuning (PEFT) techniques intro-\\nduce a small set of trainable parameters or learnable prompts\\nwhile leaving most of the model weights frozen. Approaches\\nsuch as LoRA [60], Prefix Tuning [231], and Adapters [232]\\nexemplify this strategy by injecting lightweight modules (or\\nprompts) in specific layers, thus significantly reducing the\\nmemory footprint.\\nFigure 4 illustrates how these techniques fit into a broader\\necosystemthatinvolvessystem-leveloptimizations,dataman-\\nagement, and evaluation strategies for LLMs. In particular,\\nPEFT approaches can be combined with quantization and\\npruning methods [190, 188] to further minimize memory\\nusage and compute overhead, enabling finetuning on smaller\\nGPUs or even consumer-grade hardware. For instance,QLoRA\\nunifies 4-bit quantization with low-rank adaptation, while\\nBitsAndBytes provides 8-bit optimizers to makeLLM training\\nmore practical in constrained environments (Table 2).\\nMoreover, these PEFT methods still require supervised\\ndata to guide the adaptation process, but the reduction in\\nthe number of trainable parameters makes it more feasible\\nto use in-domain or task-specific datasets. This is especially\\nvaluable for specialized domains (e.g., medical or software\\ndevelopment), where data might be limited or expensive to\\nannotate. As shown in Table 2,PEFT (HF) integrates several\\nof these approaches (LoRA, prefix tuning, and more) into a\\nsingle library, streamlining deployment in both research and\\nproduction settings.\\nCombining efficient tuning designs likeLoRA\\nand QLoRA with system and data optimizations\\n(Figure4)enablescost-effectiveLLMadaptation\\nfor tasks like domain-specific text generation,\\nwithout expensive full fine-tuning.\\n5 Test-time Scaling Methods\\nWhileRL fine-tunes the model’s policy, test-time scaling (TTS)\\nenhances reasoning during inference typically without model\\n\\nYou must strictly follow the below format for this task:\\n[\\n  {\\n    \"prompt\": \"Your generated prompt\",\\n    \"chosen\": \"Chosen completion text\",\\n    \"rejected\": \"Rejected completion text\"\\n  },\\n  ...\\n]\\n\\nNotes:\\n- Both \"prompt\", \"chosen\" and \"rejected\" fields must be non-empty. \"Chosen\" answer must be in high quality and long enough.\\n- Each sample must be a JSON dictionary with two keys: \"prompt\" and \"completion\".\\n- You MUST ONLY return the output text with the above format and nothing else.\\n\\nAdditional Dataset Info: I want to use your outputs to train a AI Researcher Model\\n'}, {'role': 'assistant', 'content': '```json\\n[\\n  {\\n    \"prompt\": \"Hãy giải thích ngắn gọn về LoRA và QLoRA trong lĩnh vực tinh chỉnh mô hình ngôn ngữ lớn (LLM).\",\\n    \"chosen\": \"LoRA (Low-Rank Adaptation) là một phương pháp tinh chỉnh hiệu quả tham số, bằng cách chèn các adapter có thứ hạng thấp có thể huấn luyện được vào mô hình. QLoRA (Quantized Adaptation) kết hợp lượng tử hóa 4-bit với LoRA, cho phép tinh chỉnh trên các GPU tiêu dùng với chi phí bộ nhớ thấp hơn đáng kể. Cả hai đều giúp giảm đáng kể lượng tài nguyên tính toán cần thiết để điều chỉnh LLM cho các tác vụ cụ thể.\",\\n    \"rejected\": \"LoRA và QLoRA là các thuật toán nén dữ liệu.\"\\n  },\\n  {\\n    \"prompt\": \"Mô tả vai trò của HF Datasets và WebDataset trong quản lý và tiền xử lý dữ liệu cho huấn luyện mô hình ngôn ngữ lớn.\",\\n    \"chosen\": \"HF Datasets cung cấp một API thống nhất cho hơn 30.000 bộ dữ liệu, hỗ trợ streaming, quản lý phiên bản và tiền xử lý dữ liệu. WebDataset là một định dạng phân mảnh dựa trên tar hiệu quả, được thiết kế cho huấn luyện phân tán quy mô petabyte. HF Datasets giúp truy cập và quản lý dữ liệu dễ dàng hơn, trong khi WebDataset tối ưu hóa hiệu suất đọc/ghi dữ liệu trong quá trình huấn luyện phân tán.\",\\n    \"rejected\": \"HF Datasets và WebDataset là các công cụ để tạo ra dữ liệu giả.\"\\n  }\\n]\\n```'}]}\n",
      "\n",
      "---\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Called get_config outside of a runnable context",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01muuid\u001b[39;00m\n\u001b[0;32m      3\u001b[0m thread_config \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfigurable\u001b[39m\u001b[38;5;124m'\u001b[39m: {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthread_id\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m123\u001b[39m}}\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m agent\u001b[38;5;241m.\u001b[39magent_flow\u001b[38;5;241m.\u001b[39mastream(\n\u001b[0;32m      6\u001b[0m     {\n\u001b[0;32m      7\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtask\u001b[39m\u001b[38;5;124m\"\u001b[39m: example_task,\n\u001b[0;32m      8\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman_feedback\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m      9\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconversations\u001b[39m\u001b[38;5;124m\"\u001b[39m: []\n\u001b[0;32m     10\u001b[0m     }\n\u001b[0;32m     11\u001b[0m     ,\n\u001b[0;32m     12\u001b[0m     config\u001b[38;5;241m=\u001b[39mthread_config, \n\u001b[0;32m     13\u001b[0m     stream_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupdates\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m     15\u001b[0m         \u001b[38;5;28mprint\u001b[39m(value)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langgraph\\pregel\\__init__.py:2062\u001b[0m, in \u001b[0;36mPregel.astream\u001b[1;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[0;32m   2056\u001b[0m \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[0;32m   2057\u001b[0m \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates\u001b[39;00m\n\u001b[0;32m   2058\u001b[0m \u001b[38;5;66;03m# channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[0;32m   2059\u001b[0m \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[0;32m   2060\u001b[0m \u001b[38;5;66;03m# with channel updates applied only at the transition between steps\u001b[39;00m\n\u001b[0;32m   2061\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtick(input_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_channels):\n\u001b[1;32m-> 2062\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner\u001b[38;5;241m.\u001b[39matick(\n\u001b[0;32m   2063\u001b[0m         loop\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[0;32m   2064\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_timeout,\n\u001b[0;32m   2065\u001b[0m         retry_policy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry_policy,\n\u001b[0;32m   2066\u001b[0m         get_waiter\u001b[38;5;241m=\u001b[39mget_waiter,\n\u001b[0;32m   2067\u001b[0m     ):\n\u001b[0;32m   2068\u001b[0m         \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[0;32m   2069\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m output():\n\u001b[0;32m   2070\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m o\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langgraph\\pregel\\runner.py:444\u001b[0m, in \u001b[0;36mPregelRunner.atick\u001b[1;34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[0m\n\u001b[0;32m    442\u001b[0m t \u001b[38;5;241m=\u001b[39m tasks[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    443\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 444\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m arun_with_retry(\n\u001b[0;32m    445\u001b[0m         t,\n\u001b[0;32m    446\u001b[0m         retry_policy,\n\u001b[0;32m    447\u001b[0m         stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_astream,\n\u001b[0;32m    448\u001b[0m         configurable\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m    449\u001b[0m             CONFIG_KEY_SEND: partial(writer, t),\n\u001b[0;32m    450\u001b[0m             CONFIG_KEY_CALL: partial(call, t),\n\u001b[0;32m    451\u001b[0m         },\n\u001b[0;32m    452\u001b[0m     )\n\u001b[0;32m    453\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    454\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langgraph\\pregel\\retry.py:128\u001b[0m, in \u001b[0;36marun_with_retry\u001b[1;34m(task, retry_policy, stream, configurable)\u001b[0m\n\u001b[0;32m    126\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 128\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m task\u001b[38;5;241m.\u001b[39mproc\u001b[38;5;241m.\u001b[39mainvoke(task\u001b[38;5;241m.\u001b[39minput, config)\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    130\u001b[0m     ns: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langgraph\\utils\\runnable.py:583\u001b[0m, in \u001b[0;36mRunnableSeq.ainvoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    579\u001b[0m config \u001b[38;5;241m=\u001b[39m patch_config(\n\u001b[0;32m    580\u001b[0m     config, callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq:step:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    581\u001b[0m )\n\u001b[0;32m    582\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 583\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m step\u001b[38;5;241m.\u001b[39mainvoke(\u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    584\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m step\u001b[38;5;241m.\u001b[39mainvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langgraph\\utils\\runnable.py:373\u001b[0m, in \u001b[0;36mRunnableCallable.ainvoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    371\u001b[0m         ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mcreate_task(coro, context\u001b[38;5;241m=\u001b[39mcontext)\n\u001b[0;32m    372\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 373\u001b[0m         ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafunc(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    374\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse:\n\u001b[0;32m    375\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m ret\u001b[38;5;241m.\u001b[39mainvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_core\\runnables\\config.py:588\u001b[0m, in \u001b[0;36mrun_in_executor\u001b[1;34m(executor_or_config, func, *args, **kwargs)\u001b[0m\n\u001b[0;32m    584\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mexc\u001b[39;00m\n\u001b[0;32m    586\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m executor_or_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(executor_or_config, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    587\u001b[0m     \u001b[38;5;66;03m# Use default executor with context copied from current context\u001b[39;00m\n\u001b[1;32m--> 588\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mget_running_loop()\u001b[38;5;241m.\u001b[39mrun_in_executor(\n\u001b[0;32m    589\u001b[0m         \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    590\u001b[0m         cast(Callable[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, T], partial(copy_context()\u001b[38;5;241m.\u001b[39mrun, wrapper)),\n\u001b[0;32m    591\u001b[0m     )\n\u001b[0;32m    593\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mget_running_loop()\u001b[38;5;241m.\u001b[39mrun_in_executor(executor_or_config, wrapper)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\concurrent\\futures\\thread.py:52\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs)\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_core\\runnables\\config.py:579\u001b[0m, in \u001b[0;36mrun_in_executor.<locals>.wrapper\u001b[1;34m()\u001b[0m\n\u001b[0;32m    577\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[0;32m    578\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 579\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    580\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    581\u001b[0m         \u001b[38;5;66;03m# StopIteration can't be set on an asyncio.Future\u001b[39;00m\n\u001b[0;32m    582\u001b[0m         \u001b[38;5;66;03m# it raises a TypeError and leaves the Future pending forever\u001b[39;00m\n\u001b[0;32m    583\u001b[0m         \u001b[38;5;66;03m# so we need to convert it to a RuntimeError\u001b[39;00m\n\u001b[0;32m    584\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mexc\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\tis\\Downloads\\syndata_agent\\src\\multi_agent.py:137\u001b[0m, in \u001b[0;36mSyntheticDataGenerator.approve\u001b[1;34m(self, currentstate)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mapprove\u001b[39m(\u001b[38;5;28mself\u001b[39m, currentstate: CurrentState) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Command:\n\u001b[0;32m    134\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;03m    Approve the first response.\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m     approval \u001b[38;5;241m=\u001b[39m \u001b[43minterrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    138\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDo you approve?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m    139\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m approval \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myes\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    141\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m Command(goto\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata_generate\u001b[39m\u001b[38;5;124m'\u001b[39m,  update\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapproval\u001b[39m\u001b[38;5;124m'\u001b[39m: approval})\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langgraph\\types.py:474\u001b[0m, in \u001b[0;36minterrupt\u001b[1;34m(value)\u001b[0m\n\u001b[0;32m    471\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlanggraph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GraphInterrupt\n\u001b[0;32m    472\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlanggraph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_config\n\u001b[1;32m--> 474\u001b[0m conf \u001b[38;5;241m=\u001b[39m \u001b[43mget_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfigurable\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    475\u001b[0m \u001b[38;5;66;03m# track interrupt index\u001b[39;00m\n\u001b[0;32m    476\u001b[0m scratchpad: PregelScratchpad \u001b[38;5;241m=\u001b[39m conf[CONFIG_KEY_SCRATCHPAD]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langgraph\\config.py:29\u001b[0m, in \u001b[0;36mget_config\u001b[1;34m()\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m var_config\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 29\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalled get_config outside of a runnable context\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Called get_config outside of a runnable context"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "\n",
    "thread_config = {'configurable': {'thread_id': 123}}\n",
    "\n",
    "async for output in agent.agent_flow.astream(\n",
    "    {\n",
    "        \"task\": example_task,\n",
    "        \"human_feedback\": None,\n",
    "        \"conversations\": []\n",
    "    }\n",
    "    ,\n",
    "    config=thread_config, \n",
    "    stream_mode=\"updates\"):\n",
    "    for key, value in output.items():\n",
    "        print(value)\n",
    "    print(\"\\n---\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.types import Command\n",
    "\n",
    "agent.agent_flow.ainvoke(\n",
    "    Command(resume='yes'),\n",
    "    config=thread_config\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
