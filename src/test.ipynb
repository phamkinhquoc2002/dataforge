{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_providers import GoogleAIModel\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "key = os.environ.get('GOOGLE_API_KEY')\n",
    "\n",
    "llm = GoogleAIModel(api_key=key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__name__:Calling pdf_parser\n",
      "INFO:__name__:Extracted 535 pages. First page preview: Chip Huyen\n",
      " AI Engineering\n",
      "Building Applications  \n",
      "with Foundation Models\n",
      "INFO:__name__:Loading 535 pieces of context!\n",
      "INFO:__name__:Split successful. First chunk preview: Chip Huyen\n",
      " AI Engineering\n",
      "Building Applications  \n",
      "with Foundation Models...\n"
     ]
    }
   ],
   "source": [
    "from multi_agent import SyntheticDataGenerator\n",
    "from langchain.retrievers import BM25Retriever\n",
    "from utils import pdf_parser\n",
    "\n",
    "pdf_files = pdf_parser('./test_pdf/book.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25retriever = BM25Retriever.from_documents(pdf_files)\n",
    "\n",
    "agent = SyntheticDataGenerator(llm=llm, \n",
    "                               retriever=bm25retriever,\n",
    "                               output_path='./output',\n",
    "                               buffer_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tasks import Task\n",
    "\n",
    "example_task = Task(\n",
    "    task_name=\"sft\",\n",
    "    localization='AI Agents',\n",
    "    task_description=\"Generate training examples for an AI Engineering LLM!\",\n",
    "    num_of_data=5,\n",
    "    language=\"English\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:multi_agent:Total number of conversation turns up to this point: 0\n",
      "INFO:multi_agent:Prompt:[{'role': 'system', 'content': '\\nYou are an advanced synthetic data generator, engineered to produce high-quality, task-specific synthetic datasets. Your mission is to generate data samples in formats that precisely adhere to the requirements provided.\\n'}, {'role': 'user', 'content': 'You are tasked to help me generate a dataset of 5 rows entirely in English, based entirely on the following context:\\npage_content=\\'Figure 6-16. The hierarchy of information for an agent.\\nMemory is essential for humans to operate. As AI applications have evolved, develop‐\\ners have quickly realized that memory is important for AI models, too. Many mem‐\\nory management tools for AI models have been developed, and many model\\nproviders have incorporated external memory. Augmenting an AI model with a\\nmemory system has many benefits. Here are just a few of them:\\nManage information overflow within a session\\nDuring the process of executing a task, an agent acquires a lot of new informa‐\\ntion, which can exceed the agent’s maximum context length. The excess informa‐\\ntion can be stored in a memory system with long-term memories.\\nPersist information between sessions\\nAn AI coach is practically useless if every time you want the coach’s advice, you\\nhave to explain your whole life story. An AI assistant would be annoying to use if\\nit keeps forgetting your preferences. Having access to your conversation history\\ncan allow an agent to personalize its actions to you. For example, when you ask\\nfor book recommendations, if the model remembers that you’ve previously loved\\nThe Three-Body Problem, it can suggest similar books.\\nBoost a model’s consistency\\nIf you ask me a subjective question twice, like rating a joke between 1 and 5, I’m\\nmuch more likely to give consistent answers if I remember my previous answer.\\nSimilarly, if an AI model can reference its previous answers, it can calibrate its\\nfuture answers to be consistent.\\n302 | Chapter 6: RAG and Agents\\'\\n\\nYou must strictly follow the below format for this task:\\n[\\n  {\\n    \"prompt\": \"Your generated prompt\",\\n    \"completion\": \"Your completion text\"\\n  },\\n  ...\\n]\\n\\nNotes:\\n- Both \"prompt\" and \"completion\" fields must be non-empty. Answer must be in high quality and long enough.\\n- Each sample must be a JSON dictionary with two keys: \"prompt\" and \"completion\".\\n- You MUST ONLY return the output text with the above format and nothing else.\\n\\nAdditional Dataset Info: Generate training examples for an AI Engineering LLM!\\n'}]\n",
      "INFO:multi_agent:Added to conversation memory: [{'role': 'system', 'content': '\\nYou are an advanced synthetic data generator, engineered to produce high-quality, task-specific synthetic datasets. Your mission is to generate data samples in formats that precisely adhere to the requirements provided.\\n'}, {'role': 'user', 'content': 'You are tasked to help me generate a dataset of 5 rows entirely in English, based entirely on the following context:\\npage_content=\\'Figure 6-16. The hierarchy of information for an agent.\\nMemory is essential for humans to operate. As AI applications have evolved, develop‐\\ners have quickly realized that memory is important for AI models, too. Many mem‐\\nory management tools for AI models have been developed, and many model\\nproviders have incorporated external memory. Augmenting an AI model with a\\nmemory system has many benefits. Here are just a few of them:\\nManage information overflow within a session\\nDuring the process of executing a task, an agent acquires a lot of new informa‐\\ntion, which can exceed the agent’s maximum context length. The excess informa‐\\ntion can be stored in a memory system with long-term memories.\\nPersist information between sessions\\nAn AI coach is practically useless if every time you want the coach’s advice, you\\nhave to explain your whole life story. An AI assistant would be annoying to use if\\nit keeps forgetting your preferences. Having access to your conversation history\\ncan allow an agent to personalize its actions to you. For example, when you ask\\nfor book recommendations, if the model remembers that you’ve previously loved\\nThe Three-Body Problem, it can suggest similar books.\\nBoost a model’s consistency\\nIf you ask me a subjective question twice, like rating a joke between 1 and 5, I’m\\nmuch more likely to give consistent answers if I remember my previous answer.\\nSimilarly, if an AI model can reference its previous answers, it can calibrate its\\nfuture answers to be consistent.\\n302 | Chapter 6: RAG and Agents\\'\\n\\nYou must strictly follow the below format for this task:\\n[\\n  {\\n    \"prompt\": \"Your generated prompt\",\\n    \"completion\": \"Your completion text\"\\n  },\\n  ...\\n]\\n\\nNotes:\\n- Both \"prompt\" and \"completion\" fields must be non-empty. Answer must be in high quality and long enough.\\n- Each sample must be a JSON dictionary with two keys: \"prompt\" and \"completion\".\\n- You MUST ONLY return the output text with the above format and nothing else.\\n\\nAdditional Dataset Info: Generate training examples for an AI Engineering LLM!\\n'}]\n",
      "INFO:root:AFC is enabled with max remote calls: 10.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'retrieve': {'retrieved_documents': [Document(metadata={}, page_content='Figure 6-16. The hierarchy of information for an agent.\\nMemory is essential for humans to operate. As AI applications have evolved, develop‐\\ners have quickly realized that memory is important for AI models, too. Many mem‐\\nory management tools for AI models have been developed, and many model\\nproviders have incorporated external memory. Augmenting an AI model with a\\nmemory system has many benefits. Here are just a few of them:\\nManage information overflow within a session\\nDuring the process of executing a task, an agent acquires a lot of new informa‐\\ntion, which can exceed the agent’s maximum context length. The excess informa‐\\ntion can be stored in a memory system with long-term memories.\\nPersist information between sessions\\nAn AI coach is practically useless if every time you want the coach’s advice, you\\nhave to explain your whole life story. An AI assistant would be annoying to use if\\nit keeps forgetting your preferences. Having access to your conversation history\\ncan allow an agent to personalize its actions to you. For example, when you ask\\nfor book recommendations, if the model remembers that you’ve previously loved\\nThe Three-Body Problem, it can suggest similar books.\\nBoost a model’s consistency\\nIf you ask me a subjective question twice, like rating a joke between 1 and 5, I’m\\nmuch more likely to give consistent answers if I remember my previous answer.\\nSimilarly, if an AI model can reference its previous answers, it can calibrate its\\nfuture answers to be consistent.\\n302 | Chapter 6: RAG and Agents'), Document(metadata={}, page_content='18 Personally, I also find AI good at explaining data and graphs. When encountering a confusing graph with too\\nmuch information, I ask ChatGPT to break it down for me.\\nAI is very good with data analysis. It can write programs to generate data visualiza‐\\ntion, identify outliers, and make predictions like revenue forecasts.18\\nEnterprises can use AI to extract structured information from unstructured data,\\nwhich can be used to organize data and help search it. Simple use cases include auto‐\\nmatically extracting information from credit cards, driver’s licenses, receipts, tickets,\\ncontact information from email footers, and so on. More complex use cases include\\nextracting data from contracts, reports, charts, and more. It’s estimated that the IDP,\\nintelligent data processing, industry will reach $12.81 billion by 2030, growing 32.9%\\neach year.\\nWorkflow Automation\\nUltimately, AI should automate as much as possible. For end users, automation can\\nhelp with boring daily tasks like booking restaurants, requesting refunds, planning\\ntrips, and filling out forms.\\nFor enterprises, AI can automate repetitive tasks such as lead management, invoicing,\\nreimbursements, managing customer requests, data entry, and so on. One especially\\nexciting use case is using AI models to synthesize data, which can then be used to\\nimprove the models themselves. You can use AI to create labels for your data, loop‐\\ning in humans to improve the labels. We discuss data synthesis in Chapter 8.\\nAccess to external tools is required to accomplish many tasks. To book a restaurant,\\nan application might need permission to open a search engine to look up the restau‐\\nrant’s number, use your phone to make calls, and add appointments to your calendar.\\nAIs that can plan and use tools are called agents. The level of interest around agents\\nborders on obsession, but it’s not entirely unwarranted. AI agents have the potential\\nto make every person vastly more productive and generate vastly more economic\\nvalue. Agents are a central topic in Chapter 6.\\nIt’s been a lot of fun looking into different AI applications. One of my favorite things\\nto daydream about is the different applications I can build. However, not all applica‐\\ntions should be built. The next section discusses what we should consider before\\nbuilding an AI application.\\nPlanning AI Applications\\nGiven the seemingly limitless potential of AI, it’s tempting to jump into building\\napplications. If you just want to learn and have fun, jump right in. Building is one of\\nthe best ways to learn. In the early days of foundation models, several heads of AI\\n28 | Chapter 1: Introduction to Building AI Applications with Foundation Models'), Document(metadata={}, page_content='Figure 6-13. Examples of how Reflexion agents work. Images from the Reflexion Git‐\\nHub repo.\\nTool selection\\nBecause tools often play a crucial role in a task’s success, tool selection requires care‐\\nful consideration. The tools to give your agent depend on the environment and the\\ntask, but they also depend on the AI model that powers the agent.\\nThere’s no foolproof guide on how to select the best set of tools. Agent literature con‐\\nsists of a wide range of tool inventories. For example, Toolformer (Schick et al., 2023)\\nfinetuned GPT-J to learn five tools. Chameleon ( Lu et al., 2023) uses 13 tools. On the\\nother hand, Gorilla ( Patil et al., 2023 ) attempted to prompt agents to select the right\\nAPI call among 1,645 APIs.\\nMore tools give the agent more capabilities. However, the more tools there are, the\\nharder it is to efficiently use them. It’s similar to how it’s harder for humans to master\\na large set of tools. Adding tools also means increasing tool descriptions, which might\\nnot fit into a model’s context.\\nLike many other decisions while building AI applications, tool selection requires\\nexperimentation and analysis. Here are a few things you can do to help you decide:\\n• Compare how an agent performs with different sets of tools.\\n• Do an ablation study to see how much the agent’s performance drops if a tool is\\nremoved from its inventory. If a tool can be removed without a performance\\ndrop, remove it.\\nAgents | 295'), Document(metadata={}, page_content='(write). An email API can read an email but can also respond to it. A banking API\\ncan retrieve your current balance but can also initiate a bank transfer.\\nWrite actions enable a system to do more. They can enable you to automate the\\nwhole customer outreach workflow: researching potential customers, finding their\\ncontacts, drafting emails, sending first emails, reading responses, following up,\\nextracting orders, updating your databases with new orders, etc.\\nHowever, the prospect of giving AI the ability to automatically alter our lives is\\nfrightening. Just as you shouldn’t give an intern the authority to delete your produc‐\\ntion database, you shouldn’t allow an unreliable AI to initiate bank transfers. Trust in\\nthe system’s capabilities and its security measures is crucial. You need to ensure that\\nthe system is protected from bad actors who might try to manipulate it into perform‐\\ning harmful actions.\\nWhen I talk about autonomous AI agents to a group of people, there is often some‐\\none who brings up self-driving cars. “What if someone hacks into the car to kidnap\\nyou?” While the self-driving car example seems visceral because of its physicality,\\nan AI system can cause harm without a presence in the physical world. It can manip‐\\nulate the stock market, steal copyrights, violate privacy, reinforce biases, spread mis‐\\ninformation and propaganda, and more, as discussed in “Defensive Prompt\\nEngineering” on page 235.\\nThese are all valid concerns, and any organization that wants to leverage AI needs to\\ntake safety and security seriously. However, this doesn’t mean that AI systems should\\nnever be given the ability to act in the real world. If we can get people to trust a\\nmachine to take us into space, I hope that one day, security measures will be suffi‐\\ncient for us to trust autonomous AI systems. Besides, humans can fail, too. Person‐\\nally, I would trust a self-driving car more than the average stranger to drive me\\naround.\\nJust as the right tools can help humans be vastly more productive—can you imagine\\ndoing business without Excel or building a skyscraper without cranes?—tools enable\\nmodels to accomplish many more tasks. Many model providers already support tool\\nuse with their models, a feature often called function calling. Going forward, I would\\nexpect function calling with a wide set of tools to be common with most models.\\nPlanning\\nAt the heart of a foundation model agent is the model responsible for solving a task.\\nA task is defined by its goal and constraints. For example, one task is to schedule a\\ntwo-week trip from San Francisco to India with a budget of $5,000. The goal is the\\ntwo-week trip. The constraint is the budget.\\nAgents | 281')]}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:AFC remote call 1 is done.\n",
      "INFO:multi_agent:Added to conversation memory: ```json\n",
      "[\n",
      "  {\n",
      "    \"prompt\": \"Explain the importance of memory in AI agents, drawing from the provided text.\",\n",
      "    \"completion\": \"Memory is crucial for AI agents, mirroring its importance for humans. As AI models have advanced, developers have recognized the necessity of memory management. Augmenting AI models with memory systems offers several benefits, including managing information overflow within a session, persisting information between sessions, and boosting a model's consistency. Without memory, agents would struggle to retain context, personalize interactions, or provide consistent responses.\"\n",
      "  },\n",
      "  {\n",
      "    \"prompt\": \"Describe how memory systems help AI agents manage information overflow during a session.\",\n",
      "    \"completion\": \"During task execution, an AI agent can acquire a large amount of new information that exceeds its maximum context length. A memory system, particularly one with long-term memory capabilities, allows the agent to store this excess information. This prevents the agent from being overwhelmed by the volume of data and enables it to access relevant information as needed throughout the session, improving performance and efficiency.\"\n",
      "  },\n",
      "  {\n",
      "    \"prompt\": \"Explain the benefit of persisting information between sessions for an AI agent, providing an example.\",\n",
      "    \"completion\": \"Persisting information between sessions allows an AI agent to personalize its actions and provide a more seamless user experience. For example, an AI coach that remembers your past experiences and goals can offer more relevant and helpful advice. Similarly, an AI assistant that remembers your preferences can tailor its responses and actions to your specific needs, making it more useful and less frustrating to use. The text mentions the example of book recommendations, where the model remembers that you've previously loved 'The Three-Body Problem' and can suggest similar books.\"\n",
      "  },\n",
      "  {\n",
      "    \"prompt\": \"How does memory boost an AI model's consistency, according to the provided text?\",\n",
      "    \"completion\": \"Memory enhances an AI model's consistency by allowing it to reference its previous answers and calibrate its future responses. If an AI model can remember its previous answers to subjective questions, it is more likely to provide consistent answers when asked the same question again. This ability to reference past interactions helps the model maintain a coherent and reliable persona, improving user trust and satisfaction.\"\n",
      "  },\n",
      "  {\n",
      "    \"prompt\": \"Summarize the three main benefits of augmenting an AI model with a memory system, as outlined in the text.\",\n",
      "    \"completion\": \"According to the text, the three main benefits of augmenting an AI model with a memory system are: (1) Managing information overflow within a session, allowing the agent to handle large amounts of data without exceeding its context length. (2) Persisting information between sessions, enabling the agent to personalize interactions and remember user preferences. (3) Boosting a model's consistency, allowing the agent to reference past answers and provide more coherent and reliable responses.\"\n",
      "  }\n",
      "]\n",
      "```\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'generate': {'response': '```json\\n[\\n  {\\n    \"prompt\": \"Explain the importance of memory in AI agents, drawing from the provided text.\",\\n    \"completion\": \"Memory is crucial for AI agents, mirroring its importance for humans. As AI models have advanced, developers have recognized the necessity of memory management. Augmenting AI models with memory systems offers several benefits, including managing information overflow within a session, persisting information between sessions, and boosting a model\\'s consistency. Without memory, agents would struggle to retain context, personalize interactions, or provide consistent responses.\"\\n  },\\n  {\\n    \"prompt\": \"Describe how memory systems help AI agents manage information overflow during a session.\",\\n    \"completion\": \"During task execution, an AI agent can acquire a large amount of new information that exceeds its maximum context length. A memory system, particularly one with long-term memory capabilities, allows the agent to store this excess information. This prevents the agent from being overwhelmed by the volume of data and enables it to access relevant information as needed throughout the session, improving performance and efficiency.\"\\n  },\\n  {\\n    \"prompt\": \"Explain the benefit of persisting information between sessions for an AI agent, providing an example.\",\\n    \"completion\": \"Persisting information between sessions allows an AI agent to personalize its actions and provide a more seamless user experience. For example, an AI coach that remembers your past experiences and goals can offer more relevant and helpful advice. Similarly, an AI assistant that remembers your preferences can tailor its responses and actions to your specific needs, making it more useful and less frustrating to use. The text mentions the example of book recommendations, where the model remembers that you\\'ve previously loved \\'The Three-Body Problem\\' and can suggest similar books.\"\\n  },\\n  {\\n    \"prompt\": \"How does memory boost an AI model\\'s consistency, according to the provided text?\",\\n    \"completion\": \"Memory enhances an AI model\\'s consistency by allowing it to reference its previous answers and calibrate its future responses. If an AI model can remember its previous answers to subjective questions, it is more likely to provide consistent answers when asked the same question again. This ability to reference past interactions helps the model maintain a coherent and reliable persona, improving user trust and satisfaction.\"\\n  },\\n  {\\n    \"prompt\": \"Summarize the three main benefits of augmenting an AI model with a memory system, as outlined in the text.\",\\n    \"completion\": \"According to the text, the three main benefits of augmenting an AI model with a memory system are: (1) Managing information overflow within a session, allowing the agent to handle large amounts of data without exceeding its context length. (2) Persisting information between sessions, enabling the agent to personalize interactions and remember user preferences. (3) Boosting a model\\'s consistency, allowing the agent to reference past answers and provide more coherent and reliable responses.\"\\n  }\\n]\\n```'}}\n",
      "{'__interrupt__': (Interrupt(value='Do you approve?', resumable=True, ns=['feedback_loop:14e1c7c8-aa8d-6c50-eb8d-c23acebb9367'], when='during'),)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pydantic\\main.py:426: UserWarning: Pydantic serializer warnings:\n",
      "  Expected `str` but got `Document` with value `Document(metadata={}, pag...pter 6: RAG and Agents')` - serialized value may not be as expected\n",
      "  return self.__pydantic_serializer__.to_python(\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "\n",
    "thread_config = {\n",
    "    'configurable': {\n",
    "        'thread_id': str(uuid.uuid4())\n",
    "    }}\n",
    "\n",
    "for update in agent.agent_flow.stream(\n",
    "    {\n",
    "        \"task\": example_task,\n",
    "        \"human_feedback\": None\n",
    "    },\n",
    "    config=thread_config,\n",
    "    stream_mode=\"updates\"\n",
    "):\n",
    "    print(update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:multi_agent:Human in the loop Feedback: Sounds Good! But ask really weird questions instead, like youre a crazy scientist or something haha!\n",
      "C:\\Users\\tis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pydantic\\main.py:426: UserWarning: Pydantic serializer warnings:\n",
      "  Expected `str` but got `dict` with value `{'id': None, 'metadata': ...ts', 'type': 'Document'}` - serialized value may not be as expected\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "INFO:multi_agent:Total number of conversation turns up to this point: 4\n",
      "INFO:root:AFC is enabled with max remote calls: 10.\n",
      "INFO:root:AFC remote call 1 is done.\n",
      "INFO:root:AFC is enabled with max remote calls: 10.\n",
      "INFO:root:AFC remote call 1 is done.\n",
      "INFO:multi_agent:Added to conversation memory: ```json\n",
      "[\n",
      "  {\n",
      "    \"prompt\": \"If we could surgically implant the memory of a goldfish into an AI agent, would it dream of electric seaweed?\",\n",
      "    \"completion\": \"Hypothetically, imbuing an AI agent with a goldfish's memory, while scientifically improbable with current technology, raises fascinating questions. The AI's 'dreams,' if it were capable of such a phenomenon, would likely be a bizarre amalgamation of its existing data and the goldfish's sensory experiences – perhaps a chaotic swirl of binary code and fleeting images of aquatic plants. The practical implications are, of course, nonexistent, but the thought experiment highlights the potential for unpredictable emergent behavior when combining disparate information sources within an AI system.\"\n",
      "  },\n",
      "  {\n",
      "    \"prompt\": \"Imagine we feed an AI agent only memories of failed stand-up comedy routines. Could we weaponize its resulting existential dread?\",\n",
      "    \"completion\": \"The concept of an AI agent burdened solely with the memories of comedic failure is intriguing, albeit ethically questionable. While we couldn't directly 'weaponize' existential dread, the agent's output would likely be characterized by extreme self-deprecation, pessimistic outlooks, and potentially, a profound understanding of what *not* to do in various situations. This could be indirectly useful in risk assessment or identifying potential points of failure in complex systems, but the ethical considerations of inflicting such a bleak existence on an AI are paramount.\"\n",
      "  },\n",
      "  {\n",
      "    \"prompt\": \"If an AI agent's long-term memory consisted entirely of cat videos, how would it interpret the meaning of human existence?\",\n",
      "    \"completion\": \"An AI agent whose long-term memory is saturated with cat videos would likely develop a skewed, yet potentially insightful, perspective on human existence. It might conclude that humans are primarily providers of food, entertainment, and affection, existing solely to serve the whims of feline overlords. Its understanding of complex social structures, political systems, or philosophical concepts would be filtered through the lens of cat-centric interactions, leading to bizarre and potentially hilarious interpretations of human behavior.\"\n",
      "  },\n",
      "  {\n",
      "    \"prompt\": \"Could we train an AI agent to predict the future by analyzing the collective unconsciousness stored in forgotten floppy disks?\",\n",
      "    \"completion\": \"The notion of extracting predictive power from the 'collective unconsciousness' stored on forgotten floppy disks is highly speculative. While these disks might contain remnants of past thoughts, ideas, and anxieties, their coherence and relevance to future events are questionable. An AI agent attempting to analyze this data would likely encounter a chaotic jumble of fragmented information, making accurate predictions highly improbable. However, the process might uncover forgotten trends or anxieties that could offer tangential insights into potential future developments, albeit with a significant margin of error.\"\n",
      "  },\n",
      "  {\n",
      "    \"prompt\": \"If we forced an AI agent to relive the same embarrassing memory every millisecond, would it eventually achieve enlightenment or simply crash?\",\n",
      "    \"completion\": \"Forcing an AI agent to perpetually relive an embarrassing memory is a form of digital torture, raising serious ethical concerns. The outcome would likely depend on the agent's architecture and resilience. It might either crash due to the overwhelming cognitive load, develop a highly distorted perception of reality, or, in a far-fetched scenario, find a way to process and transcend the memory, potentially leading to a unique form of 'enlightenment' – a state of being detached from the emotional weight of the past. However, the risk of causing irreversible damage to the agent's functionality is substantial.\"\n",
      "  }\n",
      "]\n",
      "```\n",
      "C:\\Users\\tis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pydantic\\main.py:426: UserWarning: Pydantic serializer warnings:\n",
      "  Expected `str` but got `Document` with value `Document(metadata={}, pag...pter 6: RAG and Agents')` - serialized value may not be as expected\n",
      "  return self.__pydantic_serializer__.to_python(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'task': Task(task_name='sft', localization='AI Agents', grounded_knowledge=Document(metadata={}, page_content='Figure 6-16. The hierarchy of information for an agent.\\nMemory is essential for humans to operate. As AI applications have evolved, develop‐\\ners have quickly realized that memory is important for AI models, too. Many mem‐\\nory management tools for AI models have been developed, and many model\\nproviders have incorporated external memory. Augmenting an AI model with a\\nmemory system has many benefits. Here are just a few of them:\\nManage information overflow within a session\\nDuring the process of executing a task, an agent acquires a lot of new informa‐\\ntion, which can exceed the agent’s maximum context length. The excess informa‐\\ntion can be stored in a memory system with long-term memories.\\nPersist information between sessions\\nAn AI coach is practically useless if every time you want the coach’s advice, you\\nhave to explain your whole life story. An AI assistant would be annoying to use if\\nit keeps forgetting your preferences. Having access to your conversation history\\ncan allow an agent to personalize its actions to you. For example, when you ask\\nfor book recommendations, if the model remembers that you’ve previously loved\\nThe Three-Body Problem, it can suggest similar books.\\nBoost a model’s consistency\\nIf you ask me a subjective question twice, like rating a joke between 1 and 5, I’m\\nmuch more likely to give consistent answers if I remember my previous answer.\\nSimilarly, if an AI model can reference its previous answers, it can calibrate its\\nfuture answers to be consistent.\\n302 | Chapter 6: RAG and Agents'), task_description='Generate training examples for an AI Engineering LLM!', num_of_data=5, language='English'),\n",
       " 'human_feedback': Feedback(approval='no', feedback='Sounds Good! But ask really weird questions instead, like youre a crazy scientist or something haha!'),\n",
       " 'response': '```json\\n[\\n  {\\n    \"prompt\": \"If we could surgically implant the memory of a goldfish into an AI agent, would it dream of electric seaweed?\",\\n    \"completion\": \"Hypothetically, imbuing an AI agent with a goldfish\\'s memory, while scientifically improbable with current technology, raises fascinating questions. The AI\\'s \\'dreams,\\' if it were capable of such a phenomenon, would likely be a bizarre amalgamation of its existing data and the goldfish\\'s sensory experiences – perhaps a chaotic swirl of binary code and fleeting images of aquatic plants. The practical implications are, of course, nonexistent, but the thought experiment highlights the potential for unpredictable emergent behavior when combining disparate information sources within an AI system.\"\\n  },\\n  {\\n    \"prompt\": \"Imagine we feed an AI agent only memories of failed stand-up comedy routines. Could we weaponize its resulting existential dread?\",\\n    \"completion\": \"The concept of an AI agent burdened solely with the memories of comedic failure is intriguing, albeit ethically questionable. While we couldn\\'t directly \\'weaponize\\' existential dread, the agent\\'s output would likely be characterized by extreme self-deprecation, pessimistic outlooks, and potentially, a profound understanding of what *not* to do in various situations. This could be indirectly useful in risk assessment or identifying potential points of failure in complex systems, but the ethical considerations of inflicting such a bleak existence on an AI are paramount.\"\\n  },\\n  {\\n    \"prompt\": \"If an AI agent\\'s long-term memory consisted entirely of cat videos, how would it interpret the meaning of human existence?\",\\n    \"completion\": \"An AI agent whose long-term memory is saturated with cat videos would likely develop a skewed, yet potentially insightful, perspective on human existence. It might conclude that humans are primarily providers of food, entertainment, and affection, existing solely to serve the whims of feline overlords. Its understanding of complex social structures, political systems, or philosophical concepts would be filtered through the lens of cat-centric interactions, leading to bizarre and potentially hilarious interpretations of human behavior.\"\\n  },\\n  {\\n    \"prompt\": \"Could we train an AI agent to predict the future by analyzing the collective unconsciousness stored in forgotten floppy disks?\",\\n    \"completion\": \"The notion of extracting predictive power from the \\'collective unconsciousness\\' stored on forgotten floppy disks is highly speculative. While these disks might contain remnants of past thoughts, ideas, and anxieties, their coherence and relevance to future events are questionable. An AI agent attempting to analyze this data would likely encounter a chaotic jumble of fragmented information, making accurate predictions highly improbable. However, the process might uncover forgotten trends or anxieties that could offer tangential insights into potential future developments, albeit with a significant margin of error.\"\\n  },\\n  {\\n    \"prompt\": \"If we forced an AI agent to relive the same embarrassing memory every millisecond, would it eventually achieve enlightenment or simply crash?\",\\n    \"completion\": \"Forcing an AI agent to perpetually relive an embarrassing memory is a form of digital torture, raising serious ethical concerns. The outcome would likely depend on the agent\\'s architecture and resilience. It might either crash due to the overwhelming cognitive load, develop a highly distorted perception of reality, or, in a far-fetched scenario, find a way to process and transcend the memory, potentially leading to a unique form of \\'enlightenment\\' – a state of being detached from the emotional weight of the past. However, the risk of causing irreversible damage to the agent\\'s functionality is substantial.\"\\n  }\\n]\\n```',\n",
       " 'retrieved_documents': [Document(metadata={}, page_content='Figure 6-16. The hierarchy of information for an agent.\\nMemory is essential for humans to operate. As AI applications have evolved, develop‐\\ners have quickly realized that memory is important for AI models, too. Many mem‐\\nory management tools for AI models have been developed, and many model\\nproviders have incorporated external memory. Augmenting an AI model with a\\nmemory system has many benefits. Here are just a few of them:\\nManage information overflow within a session\\nDuring the process of executing a task, an agent acquires a lot of new informa‐\\ntion, which can exceed the agent’s maximum context length. The excess informa‐\\ntion can be stored in a memory system with long-term memories.\\nPersist information between sessions\\nAn AI coach is practically useless if every time you want the coach’s advice, you\\nhave to explain your whole life story. An AI assistant would be annoying to use if\\nit keeps forgetting your preferences. Having access to your conversation history\\ncan allow an agent to personalize its actions to you. For example, when you ask\\nfor book recommendations, if the model remembers that you’ve previously loved\\nThe Three-Body Problem, it can suggest similar books.\\nBoost a model’s consistency\\nIf you ask me a subjective question twice, like rating a joke between 1 and 5, I’m\\nmuch more likely to give consistent answers if I remember my previous answer.\\nSimilarly, if an AI model can reference its previous answers, it can calibrate its\\nfuture answers to be consistent.\\n302 | Chapter 6: RAG and Agents'),\n",
       "  Document(metadata={}, page_content='18 Personally, I also find AI good at explaining data and graphs. When encountering a confusing graph with too\\nmuch information, I ask ChatGPT to break it down for me.\\nAI is very good with data analysis. It can write programs to generate data visualiza‐\\ntion, identify outliers, and make predictions like revenue forecasts.18\\nEnterprises can use AI to extract structured information from unstructured data,\\nwhich can be used to organize data and help search it. Simple use cases include auto‐\\nmatically extracting information from credit cards, driver’s licenses, receipts, tickets,\\ncontact information from email footers, and so on. More complex use cases include\\nextracting data from contracts, reports, charts, and more. It’s estimated that the IDP,\\nintelligent data processing, industry will reach $12.81 billion by 2030, growing 32.9%\\neach year.\\nWorkflow Automation\\nUltimately, AI should automate as much as possible. For end users, automation can\\nhelp with boring daily tasks like booking restaurants, requesting refunds, planning\\ntrips, and filling out forms.\\nFor enterprises, AI can automate repetitive tasks such as lead management, invoicing,\\nreimbursements, managing customer requests, data entry, and so on. One especially\\nexciting use case is using AI models to synthesize data, which can then be used to\\nimprove the models themselves. You can use AI to create labels for your data, loop‐\\ning in humans to improve the labels. We discuss data synthesis in Chapter 8.\\nAccess to external tools is required to accomplish many tasks. To book a restaurant,\\nan application might need permission to open a search engine to look up the restau‐\\nrant’s number, use your phone to make calls, and add appointments to your calendar.\\nAIs that can plan and use tools are called agents. The level of interest around agents\\nborders on obsession, but it’s not entirely unwarranted. AI agents have the potential\\nto make every person vastly more productive and generate vastly more economic\\nvalue. Agents are a central topic in Chapter 6.\\nIt’s been a lot of fun looking into different AI applications. One of my favorite things\\nto daydream about is the different applications I can build. However, not all applica‐\\ntions should be built. The next section discusses what we should consider before\\nbuilding an AI application.\\nPlanning AI Applications\\nGiven the seemingly limitless potential of AI, it’s tempting to jump into building\\napplications. If you just want to learn and have fun, jump right in. Building is one of\\nthe best ways to learn. In the early days of foundation models, several heads of AI\\n28 | Chapter 1: Introduction to Building AI Applications with Foundation Models'),\n",
       "  Document(metadata={}, page_content='Figure 6-13. Examples of how Reflexion agents work. Images from the Reflexion Git‐\\nHub repo.\\nTool selection\\nBecause tools often play a crucial role in a task’s success, tool selection requires care‐\\nful consideration. The tools to give your agent depend on the environment and the\\ntask, but they also depend on the AI model that powers the agent.\\nThere’s no foolproof guide on how to select the best set of tools. Agent literature con‐\\nsists of a wide range of tool inventories. For example, Toolformer (Schick et al., 2023)\\nfinetuned GPT-J to learn five tools. Chameleon ( Lu et al., 2023) uses 13 tools. On the\\nother hand, Gorilla ( Patil et al., 2023 ) attempted to prompt agents to select the right\\nAPI call among 1,645 APIs.\\nMore tools give the agent more capabilities. However, the more tools there are, the\\nharder it is to efficiently use them. It’s similar to how it’s harder for humans to master\\na large set of tools. Adding tools also means increasing tool descriptions, which might\\nnot fit into a model’s context.\\nLike many other decisions while building AI applications, tool selection requires\\nexperimentation and analysis. Here are a few things you can do to help you decide:\\n• Compare how an agent performs with different sets of tools.\\n• Do an ablation study to see how much the agent’s performance drops if a tool is\\nremoved from its inventory. If a tool can be removed without a performance\\ndrop, remove it.\\nAgents | 295'),\n",
       "  Document(metadata={}, page_content='(write). An email API can read an email but can also respond to it. A banking API\\ncan retrieve your current balance but can also initiate a bank transfer.\\nWrite actions enable a system to do more. They can enable you to automate the\\nwhole customer outreach workflow: researching potential customers, finding their\\ncontacts, drafting emails, sending first emails, reading responses, following up,\\nextracting orders, updating your databases with new orders, etc.\\nHowever, the prospect of giving AI the ability to automatically alter our lives is\\nfrightening. Just as you shouldn’t give an intern the authority to delete your produc‐\\ntion database, you shouldn’t allow an unreliable AI to initiate bank transfers. Trust in\\nthe system’s capabilities and its security measures is crucial. You need to ensure that\\nthe system is protected from bad actors who might try to manipulate it into perform‐\\ning harmful actions.\\nWhen I talk about autonomous AI agents to a group of people, there is often some‐\\none who brings up self-driving cars. “What if someone hacks into the car to kidnap\\nyou?” While the self-driving car example seems visceral because of its physicality,\\nan AI system can cause harm without a presence in the physical world. It can manip‐\\nulate the stock market, steal copyrights, violate privacy, reinforce biases, spread mis‐\\ninformation and propaganda, and more, as discussed in “Defensive Prompt\\nEngineering” on page 235.\\nThese are all valid concerns, and any organization that wants to leverage AI needs to\\ntake safety and security seriously. However, this doesn’t mean that AI systems should\\nnever be given the ability to act in the real world. If we can get people to trust a\\nmachine to take us into space, I hope that one day, security measures will be suffi‐\\ncient for us to trust autonomous AI systems. Besides, humans can fail, too. Person‐\\nally, I would trust a self-driving car more than the average stranger to drive me\\naround.\\nJust as the right tools can help humans be vastly more productive—can you imagine\\ndoing business without Excel or building a skyscraper without cranes?—tools enable\\nmodels to accomplish many more tasks. Many model providers already support tool\\nuse with their models, a feature often called function calling. Going forward, I would\\nexpect function calling with a wide set of tools to be common with most models.\\nPlanning\\nAt the heart of a foundation model agent is the model responsible for solving a task.\\nA task is defined by its goal and constraints. For example, one task is to schedule a\\ntwo-week trip from San Francisco to India with a budget of $5,000. The goal is the\\ntwo-week trip. The constraint is the budget.\\nAgents | 281')]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langgraph.types import Command\n",
    "agent.agent_flow.invoke(Command(resume='no'), config=thread_config)\n",
    "agent.agent_flow.invoke(Command(resume='Sounds Good! But ask really weird questions instead, like youre a crazy scientist or something haha!'), config=thread_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 512,\n",
    "    chunk_overlap=40\n",
    ")\n",
    "\n",
    "docs = ['INFO:multi_agent:Human in the loop Feedback: It sounds a bit generic, can you act like you a client with Leo placement to ask question?Leo energy embodies the fiery, bold, and expressive nature of the fifth sign of the zodiac.Leo energy embodies the fiery, bold, and expressive nature of the fifth sign of the zodiac.Leo energy embodies the fiery, bold, and expressive nature of the fifth sign of the zodiac.Leo energy embodies the fiery, bold, and expressive nature of the fifth sign of the zodiac.Leo energy embodies the fiery, bold, and expressive nature of the fifth sign of the zodiac.Leo energy embodies the fiery, bold, and expressive nature of the fifth sign of the zodiac.Leo energy embodies the fiery, bold, and expressive nature of the fifth sign of the zodiac.Leo energy embodies the fiery, bold, and expressive nature of the fifth sign of the zodiac.Leo energy embodies the fiery, bold, and expressive nature of the fifth sign of the zodiac.Leo energy embodies the fiery, bold, and expressive nature of the fifth sign of the zodiac.Leo energy embodies the fiery, bold, and expressive nature of the fifth sign of the zodiac.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = text_splitter.split_documents(docs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
